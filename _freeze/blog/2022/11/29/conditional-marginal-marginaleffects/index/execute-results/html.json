{
  "hash": "78e882e27a61fd415b129bddaffa66f9",
  "result": {
    "markdown": "---\ntitle: \"Marginal and conditional effects for GLMMs with {marginaleffects}\"\ndate: 2022-11-29\ndescription: \"Use the {marginaleffects} package to calculate tricky and nuanced marginal and conditional effects in generalized linear mixed models\"\nimage: index_files/figure-html/plot-conditional-preds-1.png\ncategories:\n  - r\n  - tidyverse\n  - regression\n  - statistics\n  - bayes\n  - brms\n  - lognormal\nresources: \n  - \"df_example_lognormal.rds\"\nformat:\n  html: \n    code-fold: false\n    fig-cap-location: bottom\n    shift-heading-level-by: 1\ndoi: 10.59350/xwnfm-x1827\ncitation: true\n---\n\n\n\n\nAs a field, statistics is really bad at naming things. \n\nTake, for instance, the term \"fixed effects.\" In econometrics and other social science-flavored statistics, this typically refers to categorical terms in a regression model. Like, if we run a model like this with [gapminder](https://github.com/jennybc/gapminder) data…\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(gapminder)\n\nsome_model <- lm(lifeExp ~ gdpPercap + country,\n                 data = gapminder)\n```\n:::\n\n\n…we can say that we've added \"country fixed effects.\"\n\nThat's all fine and good until we come to the world of hierarchical or multilevel models, which has its own issues with nomenclature and can't decide what to even call itself:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Image by [Chelsea Parlett-Pelleriti](https://twitter.com/chelseaparlett/status/1458461737431146500)](chelsea-meme.jpg){fig-align='center' width=60%}\n:::\n:::\n\n\nIf we fit a model like this with country-based offsets to the intercept…\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(lme4)\n\nsome_multilevel_model <- lmer(lifeExp ~ gdpPercap + (1 | country), \n                              data = gapminder)\n```\n:::\n\n\n…then we get to say that there are \"country random effects\" or \"country group effects\", while `gdpPercap` is actually a \"fixed effect\" or \"population-level effect\" \n\n\"Fixed effects\" in multilevel models aren't at all the same as \"fixed effects\" in econometrics-land. \n\nWild.\n\n\n::: {.cell .column-page layout-align=\"center\"}\n::: {.cell-output-display}\n![lol statisticians call opposite things the same thing](fixed-random-effects.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\nAnother confusing term is the idea of \"marginal effects.\" One common definition of marginal effects is that they are slopes, or [as the {marginaleffects} vignette says](https://vincentarelbundock.github.io/marginaleffects/articles/marginaleffects.html)…\n\n> …partial derivatives of the regression equation with respect to each variable in the model for each unit in the data.\n\nThere's a whole R package ({marginaleffects}) dedicated to calculating these, and [I have a whole big long guide about this](https://www.andrewheiss.com/blog/2022/05/20/marginalia/). Basically marginal effects are the change in the outcome in a regression model when you move one of the explanatory variables up a little while holding all other covariates constant.\n\nBut there's also another definition (seemingly?) unrelated to the idea of partial derivatives or slopes! And once again, it's a key part of the multilevel model world. I've run into it many times when reading about multilevel models (and I've even kind of alluded to it [in past blog posts like this](https://www.andrewheiss.com/blog/2021/11/10/ame-bayes-re-guide/)), but I've never fully understood what multilevel marginal effects are and how they're different from slope-based marginal effects.\n\nIn multilevel models, you can calculate both *marginal effects* and *conditional effects*. Neither are necessarily related to slopes (though they both can be). They're often mixed up. Even {brms} used to have a function named `marginal_effects()` that they've renamed to `conditional_effects()`.\n\nI'm not alone in my inability to remember the difference between marginal and conditional effects in multilevel models, it seems. Everyone mixes these up. [TJ Mahr recently tweeted about the confusion](https://twitter.com/tjmahr/status/1581563839459385344):\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](tj-conditional-marginal-tweet.png){fig-align='center' width=85%}\n:::\n:::\n\n\nTJ studies language development in children and often works with data with repeated child subjects. His typical models might look something like this, with observations grouped by child:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntj_model <- lmer(y ~ x1 + x2 + (1 | child),\n                 data = whatever)\n```\n:::\n\n\nHis data has child-based clusters, since individual children have repeated observations over time. We can find two different kinds of effects given this type of multilevel model: we can look at the effect of `x1` or `x2` in one typical child, or we can look at the effect of `x1` or `x2` across all children on average. The confusingly-named terms \"conditional effect\" and \"marginal effect\" refer to each of these \"flavors\" of effect:\n\n- **Conditional effect** = average child\n- **Marginal effect** = children on average\n\nIf we have country random effects like `(1 | country)` like I do in my own work, we can calculate the same two kinds of effects. Imagine a multilevel model like this:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(lme4)\n\nsome_multilevel_model <- lmer(lifeExp ~ gdpPercap + (1 | country), \n                              data = gapminder)\n```\n:::\n\n\nOr more formally,\n\n$$\n\\begin{aligned}\n\\text{lifeExp} &\\sim \\mathcal{N}(\\mu_{i_j}, \\sigma_y) & \\text{Life expectancy within countries } j \\\\\n\\mu_{i_j} &= (\\beta_0 + b_{0_j}) + \\beta_1\\, \\text{gdpPercap}_{i_j} & \\text{Model of within-country variation} \\\\\nb_{0_j} &\\sim \\mathcal{N}(0, \\sigma_0) & \\text{Random country offsets from global average}\n\\end{aligned}\n$$\n\nWith this model, we can look at two different types of effects:\n\n- **Conditional effect** = effect of `gdpPercap` ($\\beta_1$) in an average or typical country (where the random country offset $b_{0_j}$ is 0)\n- **Marginal effect** = average effect of `gdpPercap` ($\\beta_1$ again) across all countries (where the random country offset $b_{0_j}$ is dealt with… somehow…)\n\nThis conditional vs. marginal distinction applies to any sort of hierarchical structure in multilevel models:\n\n- **Conditional effect** = group-specific, subject-specific, cluster-specific, country-specific effect. We set all group-specific random offsets to 0 to find the effect for a *typical* group / subject / student / child / cluster / country / whatever.\n- **Marginal effect** = global population-level average effect, or global effect, where group-specific differences are averaged out or integrated out or held constant.\n\nCalculating these different effects can be tricky, even with OLS-like normal or Gaussian regression, and interpreting them can get extra complicated with generalized linear mixed models (GLMMs) where we use links like Poisson, negative binomial, logistic, or lognormal families. The math with GLMMs gets *complicated*—particularly with lognormal models. [Kristoffer Magnusson has several incredible blog posts](https://rpsychologist.com/GLMM-part1-lognormal) that explore the exact math behind each of these effects in a lognormal GLMM.\n\nVincent Arel-Bundock's magisterial [{marginaleffects}](https://vincentarelbundock.github.io/marginaleffects/) R package can calculate both conditional and marginal effects automatically. I accidentally stumbled across the idea of multilevel marginal and conditional effects [in an earlier blog post](https://www.andrewheiss.com/blog/2021/11/10/ame-bayes-re-guide/#overall-summary-of-different-approaches), but there I did everything with {emmeans} rather than {marginaleffects}, and [as I explore here](https://www.andrewheiss.com/blog/2022/05/20/marginalia/), {marginaleffects} is great for calculating average marginal effects (AMEs) rather than marginal effects at the mean (MEMs). Also in that earlier guide, I don't really use this \"conditional\" vs. \"marginal\" distinction and just end up calling everything marginal. So everything here is more in line with the seemingly standard multilevel model ideas of \"conditional\" and \"marginal\" effects. \n\nLet's load some libraries, use some neat colors and a nice ggplot theme, and get started.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(tidybayes)\nlibrary(marginaleffects)\nlibrary(broom.mixed)\nlibrary(kableExtra)\nlibrary(scales)\nlibrary(ggtext)\nlibrary(patchwork)\n\n# Southern Utah colors\nclrs <- NatParksPalettes::natparks.pals(\"BryceCanyon\")\n\n# Custom ggplot themes to make pretty plots\n# Get Noto Sans at https://fonts.google.com/specimen/Noto+Sans\ntheme_nice <- function() {\n  theme_bw(base_family = \"Noto Sans\") +\n    theme(panel.grid.minor = element_blank(),\n          plot.background = element_rect(fill = \"white\", color = NA),\n          plot.title = element_text(face = \"bold\"),\n          strip.text = element_text(face = \"bold\"),\n          strip.background = element_rect(fill = \"grey80\", color = NA),\n          legend.title = element_text(face = \"bold\"))\n}\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n# Magnusson's data and model: the effect of a treatment on gambling losses\n\nTo make sure I've translated [Magnusson's math](https://rpsychologist.com/GLMM-part1-lognormal) into the corresponding (and correct) {marginaleffects} syntax, I recreate his analysis here. He imagines some sort of intervention or treatment $\\text{TX}$ that is designed to reduce the amount of dollars lost in gambling each week ($Y$). The individuals in this situation are grouped into some sort of clusters—perhaps neighborhoods, states, or countries, or even the same individuals over time if we have repeated longitudinal observations. The exact kind of cluster doesn't matter here—all that matters is that observations are nested in groups, and those groups have their own specific characteristics that influence individual-level outcomes. In this simulated data, there are 20 clusters, with 30 individuals in each cluster, with 600 total observations.\n\nTo be more formal about the structure, we can say that every outcome $Y$ gets two subscripts for the cluster ($j$) and person inside each cluster ($i_j$). We thus have $Y_{i_j}$ where $i_j \\in \\{1, 2, \\dots, 30\\}$ and $j \\in \\{1, 2, \\dots, 20\\}$. The nested, hierarchical, multilevel nature of the data makes the structure look something like this:\n\n\n\n\n\n::: {.column-body-outset}\n\n\n::: {.cell layout-align=\"center\" engine.opts='{\"dvisvgm.opts\":\"--font-format=woff\"}'}\n::: {.cell-output-display}\n![Individuals grouped into clusters](index_files/figure-html/clustered-structure-1.svg){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n\\ \n\nI've included Magnusson's original code for generating this data here, but you can also [download an `.rds` version of it here](df_example_lognormal.rds), or use the URL directly with `readr::read_rds()`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd <- readr::read_rds(\"https://www.andrewheiss.com/blog/2022/11/29/conditional-marginal-marginaleffects/df_example_lognormal.rds\")\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Kristoffer Magnusson's original data generation code\"}\n#' Generate lognormal data with a random intercept\n#'\n#' @param n1 patients per cluster\n#' @param n2 clusters per treatment\n#' @param B0 log intercept\n#' @param B1 log treatment effect\n#' @param sd_log log sd\n#' @param u0 SD of log intercepts (random intercept)\n#'\n#' @return a data.frame\ngen_data <- function(n1, n2, B0, B1, sd_log, u0) {\n  \n  cluster <- rep(1:(2 * n2), each = n1)\n  TX <- rep(c(0, 1), each = n1 * n2)\n  u0 <- rnorm(2 * n2, sd = u0)[cluster]\n  \n  mulog <- (B0 + B1 * TX + u0)\n  y <- rlnorm(2 * n1 * n2, meanlog = mulog, sdlog = sd_log)\n  \n  d <- data.frame(cluster,\n                  TX,\n                  y)\n  d\n}\n\nset.seed(4445)\npars <- list(\"n1\" = 30, # observations per cluster\n             \"n2\" = 10, # clusters per treatment\n             \"B0\" = log(500),\n             \"B1\" = log(0.5),\n             \"sd_log\" = 0.5,\n             \"u0\" = 0.5)\nd <- do.call(gen_data,\n             pars)\n```\n:::\n\n\nThe model of the effect of $\\text{TX}$ on gambling losses for individuals nested in clusters can be written formally like this, with cluster $j$-specific offsets to the $\\beta_0$ intercept term (i.e. $b_{0_j}$, or cluster random effects):\n\n$$\n\\begin{aligned}\n\\log (Y_{i_j}) &\\sim \\mathcal{N}(\\mu_{i_j}, \\sigma_y) & \\text{Gambling losses for individual $i$ within cluster } j \\\\\n\\mu_{i_j} &= (\\beta_0 + b_{0_j}) + \\beta_1\\, \\text{TX}_{i_j} & \\text{Model of within-cluster variation} \\\\\nb_{0_j} &\\sim \\mathcal{N}(0, \\sigma_0) & \\text{Random cluster offsets from global average}\n\\end{aligned}\n$$\n\nWe can fit this model with {brms} (or `lme4::lmer()` if you don't want to be Bayesian):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit <- brm(\n  bf(y ~ 1 + TX + (1 | cluster)), \n  family = lognormal(), \n  data = d,\n  chains = 4, iter = 5000, warmup = 1000, seed = 4445\n)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit\n##  Family: lognormal \n##   Links: mu = identity; sigma = identity \n## Formula: y ~ 1 + TX + (1 | cluster) \n##    Data: dat (Number of observations: 600) \n##   Draws: 4 chains, each with iter = 5000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 16000\n## \n## Group-Level Effects: \n## ~cluster (Number of levels: 20) \n##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sd(Intercept)     0.63      0.12     0.45     0.92 1.00     2024     3522\n## \n## Population-Level Effects: \n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept     6.21      0.20     5.81     6.62 1.00     2052     3057\n## TX           -0.70      0.29    -1.28    -0.13 1.00     2014     2843\n## \n## Family Specific Parameters: \n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma     0.51      0.01     0.48     0.54 1.00     7316     8256\n## \n## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n\n\nThere are four parameters that we care about in that huge wall of text. We'll pull them out as standalone objects (using [TJ Mahr's neat model-to-list trick](https://www.tjmahr.com/lists-knitr-secret-weapon/)) and show them in a table so we can keep track of everything easier.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\nr_fit <- fit %>% \n  tidy() %>% \n  mutate(term = janitor::make_clean_names(term)) %>% \n  split(~term)\n\nB0 <- r_fit$intercept$estimate\nB1 <- r_fit$tx$estimate\nsigma_y <- r_fit$sd_observation$estimate\nsigma_0 <- r_fit$sd_intercept$estimate\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nfit %>% \n  tidy() %>% \n  mutate(Parameter = c(\"\\\\(\\\\beta_0\\\\)\", \"\\\\(\\\\beta_1\\\\)\", \n                       \"\\\\(\\\\sigma_0\\\\)\", \"\\\\(\\\\sigma_y\\\\)\")) %>% \n  mutate(Description = c(\"Global average gambling losses across all individuals\",\n                         \"Effect of treatment on gambling losses for all individuals\",\n                         \"Between-cluster variability of average gambling losses\",\n                         \"Within-cluster variability of gambling losses\")) %>% \n  mutate(term = glue::glue(\"<code>{term}</code>\"),\n         estimate = round(estimate, 3)) %>% \n  select(Parameter, Term = term, Description, Estimate = estimate) %>% \n  kbl(escape = FALSE) %>% \n  kable_styling(full_width = FALSE)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Parameter </th>\n   <th style=\"text-align:left;\"> Term </th>\n   <th style=\"text-align:left;\"> Description </th>\n   <th style=\"text-align:right;\"> Estimate </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> \\(\\beta_0\\) </td>\n   <td style=\"text-align:left;\"> <code>(Intercept)</code> </td>\n   <td style=\"text-align:left;\"> Global average gambling losses across all individuals </td>\n   <td style=\"text-align:right;\"> 6.210 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> \\(\\beta_1\\) </td>\n   <td style=\"text-align:left;\"> <code>TX</code> </td>\n   <td style=\"text-align:left;\"> Effect of treatment on gambling losses for all individuals </td>\n   <td style=\"text-align:right;\"> -0.702 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> \\(\\sigma_0\\) </td>\n   <td style=\"text-align:left;\"> <code>sd__(Intercept)</code> </td>\n   <td style=\"text-align:left;\"> Between-cluster variability of average gambling losses </td>\n   <td style=\"text-align:right;\"> 0.635 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> \\(\\sigma_y\\) </td>\n   <td style=\"text-align:left;\"> <code>sd__Observation</code> </td>\n   <td style=\"text-align:left;\"> Within-cluster variability of gambling losses </td>\n   <td style=\"text-align:right;\"> 0.507 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nThere are a few problems with these estimates though: (1) they're on the log odds scale, which isn't very interpretable, and (2) neither the intercept term nor the $\\text{TX}$ term incorporate any details about the cluster-level effects beyond [the extra information we get through partial pooling](https://bayesf22-notebook.classes.andrewheiss.com/bayes-rules/16-chapter.html#shrinkage-and-the-bias-variance-tradeoff). So our goal here is to **transform these estimates into something interpretable that also incorporates group-level information**.\n\n\n# Conditional effects, or effect of a variable in an average cluster\n\n::: {.callout-tip}\n## Conditional effects\n\nConditional effects = average or typical cluster; random offsets $b_{0_j}$ set to 0\n:::\n\n**Conditional effects** refer to the effect of a variable in a  *typical* group—country, cluster, school, subject, or whatever else is in the `(1 | group)` term in the model. \"Typical\" here means that the random offset $b_{0_j}$ is set to zero, or that there are no random effects involved.\n\n## Average outcomes for a typical cluster\n\nThe average outcome $Y_{i_j}$ across the possible values of $\\text{TX}$ for a typical cluster is formally defined as\n\n$$\n\\textbf{E}(Y_{i_j} \\mid b_{0_j} = 0, \\text{TX} = \\{0, 1\\})\n$$\n\nExactly how you calculate this mathematically depends on the distribution family. For a lognormal distribution, it is this:\n\n$$\n\\textbf{E}(Y_{i_j} \\mid b_{0_j} = 0, \\text{TX} = \\{0, 1\\}) = \n\\exp \\left((\\beta_0 + b_{0_j}) + \\beta_1 \\text{TX}_i + \\frac{\\sigma_y^2}{2}\\right)\n$$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nTXs <- c(\"0\" = 0, \"1\" = 1)\nb0j <- 0\n\nexp((B0 + b0j) + (B1 * TXs) + (sigma_y^2 / 2))\n##   0   1 \n## 566 281\n```\n:::\n\n\nWe can calculate this automatically with `marginaleffects::predictions()` by setting `re_formula = NA` to ignore all random effects, or to set all the random $b_{0_j}$ offsets to zero:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredictions(\n  fit, \n  newdata = datagrid(TX = c(0, 1)), \n  by = \"TX\", \n  re_formula = NA\n)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```{.r .sourceCode}\n## # A tibble: 2 × 6\n##   rowid type        TX predicted conf.low conf.high\n##   <int> <chr>    <dbl>     <dbl>    <dbl>     <dbl>\n## 1     1 response     0      566.     379.      857.\n## 2     2 response     1      281.     188.      425.\n```\n:::\n:::\n\n\nBecause we're working with Bayesian posteriors, we might as well do neat stuff with them instead of just collapsing them down to single-number point estimates. The `posteriordraws()` function in {marginaleffects} lets us extract the modified/calculated MCMC draws, and then we can plot them with {tidybayes} / {ggdist}:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nconditional_preds <- predictions(\n  fit, \n  newdata = datagrid(TX = c(0, 1)), \n  by = \"TX\", \n  re_formula = NA\n) %>% \n  posteriordraws()\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\np_conditional_preds <- conditional_preds %>% \n  ggplot(aes(x = draw, fill = factor(TX))) +\n  stat_halfeye() +\n  scale_fill_manual(values = c(clrs[5], clrs[1])) +\n  scale_x_continuous(labels = label_dollar()) +\n  labs(x = \"Gambling losses\", y = \"Density\", fill = \"TX\",\n       title = \"Conditional cluster-specific means\",\n       subtitle = \"Typical cluster where *b*<sub>0<sub>j</sub></sub> = 0\") +\n  coord_cartesian(xlim = c(100, 1000)) +\n  theme_nice() +\n  theme(plot.subtitle = element_markdown())\np_conditional_preds\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-conditional-preds-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nNeat.\n\n## ATE for a typical cluster\n\nThe average treatment effect (ATE) for a binary treatment is the difference between the two averages when $\\text{TX} = 1$ and $\\text{TX} = 0$:\n\n$$\n\\textbf{E}(Y_{i_j} \\mid b_{0_j} = 0, \\text{TX} = 1) - \\textbf{E}(Y_{i_j} \\mid b_{0_j} = 0, \\text{TX} = 0)\n$$\n\nFor a lognormal family, it's this:\n\n$$\n\\begin{aligned}\n&\\textbf{E}(Y_{i_j} \\mid b_{0_j} = 0, \\text{TX} = 1) - \\textbf{E}(Y_{i_j} \\mid b_{0_j} = 0, \\text{TX} = 0) = \\\\\n&\\qquad \\exp \\left((\\beta_0 + b_{0_j}) + \\beta_1 + \\frac{\\sigma_y^2}{2}\\right) - \\exp \\left((\\beta_0 + b_{0_j}) + \\frac{\\sigma_y^2}{2}\\right)\n\\end{aligned}\n$$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nTXs <- c(\"0\" = 0, \"1\" = 1)\nb0j <- 0\n\n(exp((B0 + b0j) + (B1 * TXs[2]) + (sigma_y^2 / 2)) - \n    exp((B0 + b0j) + (B1 * TXs[1]) + (sigma_y^2 / 2))) %>% \n  unname()\n## [1] -286\n```\n:::\n\n\nWe can again calculate this by setting `re_formula = NA` in `marginaleffects::comparisons()`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Cluster-specific average treatment effect (when offset is 0)\ncomparisons(\n  fit, \n  variables = \"TX\",\n  re_formula = NA\n) %>% \n  tidy()\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```{.r .sourceCode}\n## # A tibble: 1 × 6\n##   type     term  contrast estimate conf.low conf.high\n##   <chr>    <chr> <chr>       <dbl>    <dbl>     <dbl>\n## 1 response TX    1 - 0       -282.    -590.     -51.3\n```\n:::\n:::\n\n\nAnd here's what the posterior of that conditional ATE looks like:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nconditional_ate <- comparisons(\n  fit, \n  variables = \"TX\",\n  re_formula = NA\n) %>% \n  posteriordraws()\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\np_conditional_ate <- conditional_ate %>% \n  ggplot(aes(x = draw)) +\n  stat_halfeye(fill = clrs[3]) +\n  scale_x_continuous(labels = label_dollar(style_negative = \"minus\")) +\n  labs(x = \"(TX = 1) − (TX = 0)\", y = \"Density\", \n       title = \"Conditional cluster-specific ATE\",\n       subtitle = \"Typical cluster where *b*<sub>0<sub>j</sub></sub> = 0\") +\n  coord_cartesian(xlim = c(-900, 300)) +\n  theme_nice() +\n  theme(plot.subtitle = element_markdown())\np_conditional_ate\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-conditional-ate-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n# Marginal effects, or effect of a variable across clusters on average\n\n::: {.callout-tip}\n## Marginal effects\n\nMarginal effects = global/population-level effect; clusters on average; random offsets $b_{0_j}$ are incorporated into the estimate\n:::\n\n**Marginal effects** refer to the global- or population-level effect of a variable. In multilevel models, coefficients can have random group-specific offsets to a global mean. That's what the $b_{0_j}$ in $(\\beta_0 + b_{0_j})$ is in the formal model we defined earlier:\n\n$$\n\\begin{aligned}\n\\mu_{i_j} &= (\\beta_0 + b_{0_j}) + \\beta_1\\, \\text{TX}_i & \\text{Model of within-cluster variation} \\\\\nb_{0_j} &\\sim \\mathcal{N}(0, \\sigma_0) & \\text{Random cluster offsets}\n\\end{aligned}\n$$\n\nBy definition, these offsets are distributed normally with a mean of 0 and a standard deviation of $\\sigma_0$, or `sd__(Intercept)` in {brms} output. We can visualize these cluster-specific offsets to get a better feel for how they work:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nfit %>% \n  linpred_draws(tibble(cluster = unique(d$cluster),\n                       TX = 0)) %>% \n  mutate(offset = B0 - .linpred) %>% \n  ungroup() %>% \n  mutate(cluster = fct_reorder(factor(cluster), offset, .fun = mean)) %>% \n  ggplot(aes(x = offset, y = cluster)) +\n  geom_vline(xintercept = 0, color = clrs[2]) +\n  stat_pointinterval(color = clrs[4]) +\n  labs(x = \"*b*<sub>0</sub> offset from β<sub>0</sub>\") +\n  theme_nice() +\n  theme(axis.title.x = element_markdown())\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-offsets-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nThe intercept for Cluster 1 here is basically the same as the global $\\beta_0$ coefficient; Cluster 19 has a big positive offset, while Cluster 11 has a big negative offset.\n\nThe model parameters show the whole range of possible cluster-specific intercepts, or $\\beta_0 \\pm \\sigma_0$:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot() +\n  stat_function(fun = ~dnorm(., mean = B0, sd = sigma_0^2),\n                geom = \"area\", fill = clrs[4]) +\n  xlim(4, 8) +\n  labs(x = \"Possible cluster-specific intercepts\", y = \"Density\",\n       title = glue::glue(\"Normal(µ = {round(B0, 3)}, σ = {round(sigma_0, 3)}<sup>2</sup>)\")) +\n  theme_nice() +\n  theme(plot.title = element_markdown())\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-offset-distribution-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nWhen generating population-level estimates, then, we need to somehow incorporate this range of possible cluster-specific intercepts into the population-level predictions. We can do this a couple different ways: we can (1) average, marginalize or integrate across them, or (2) integrate them out.\n\n\n## Average population-level outcomes\n\nThe average outcome $Y_{i_j}$ across the possible values of $\\text{TX}$ for all clusters together is formally defined as\n\n$$\n\\textbf{E}(Y_{i_j} \\mid \\text{TX} = \\{0, 1\\})\n$$\n\nAs with the conditional effects, the equation for calculating this depends on the family you're using. For lognormal families, it's this incredibly scary formula:\n\n$$\n\\textbf{E}(Y_{i_j} \\mid \\text{TX} = \\{0, 1\\}) = \\int \\exp \\left(x + \\sigma_y^2 / 2 \\right) \\, f_{\\texttt{dnorm}} \\left(x, \\left(\\beta_0 + \\beta_1 \\text{TX} \\right), \\sigma_0^2 \\right) \\,dx\n$$\n\nWild. This is a mess because it integrates over the normally-distributed cluster-specific offsets, thus incorporating them all into the overall effect.\n\nWe can calculate this integral in a few different ways. [Kristoffer Magnusson shows three different ways](https://rpsychologist.com/GLMM-part1-lognormal#how-to-calculate-marginal-effects-on-the-data-scale) to calculate this hairy integral in his original post:\n\n1. **Numeric integration** with `integrate()`:\n\n\n   ::: {.cell layout-align=\"center\"}\n   \n   ```{.r .cell-code}\n   B_TXs <- c(B0, B0 + B1) %>% set_names(c(\"0\", \"1\"))\n   \n   B_TXs %>% \n     map(~{\n       integrate(\n         f = function(x) {\n           exp(x + sigma_y ^ 2 / 2) * dnorm(x, ., sd = sigma_0)\n         },\n         lower = B0 - 10 * sigma_0,\n         upper = B0 + 10 * sigma_0\n       )$value\n     })\n   ## $`0`\n   ## [1] 692\n   ## \n   ## $`1`\n   ## [1] 343\n   ```\n   :::\n\n\n2. A magical [**moment-generating function** for the lognormal distribution](https://en.wikipedia.org/wiki/Log-normal_distribution#Characteristic_function_and_moment_generating_function):\n\n\n   ::: {.cell layout-align=\"center\"}\n   \n   ```{.r .cell-code}\n   exp(B_TXs + (sigma_0^2 + sigma_y^2)/2)\n   ##   0   1 \n   ## 692 343\n   ```\n   :::\n\n\n3. **Brute force Monte Carlo integration**, where we create a bunch of hypothetical cluster offsets $b_{0_j}$ with a mean of 0 and a standard deviation of $\\sigma_0$, calculate the average outcome, then take the average of all those hypothetical clusters:\n\n\n   ::: {.cell layout-align=\"center\"}\n   \n   ```{.r .cell-code}\n   # A bunch of hypothetical cluster offsets\n   sigma_0_i <- rnorm(1e5, 0, sigma_0)\n   B_TXs %>% \n     map(~{\n       mean(exp(. + sigma_0_i + sigma_y^2/2))\n     })\n   ## $`0`\n   ## [1] 692\n   ## \n   ## $`1`\n   ## [1] 343\n   ```\n   :::\n\n\nThose approaches are all great, but the math can get really complicated if there are interaction terms or splines or if you have more complex random effects structures (random slope offsets! nested groups!)\n\nSo instead we can use {marginaleffects} to handle all that complexity for us.\n\n4. **Average / marginalize / integrate across existing random effects**: Here we calculate predictions for $\\text{TX} = \\{0, 1\\}$ within each of the existing clusters. We then collapse them into averages for each level of $\\text{TX}$. The values here are not identical to what we found with the earlier approaches, though they're in the same general area. I'm not 100% why—I'm guessing it's because there aren't a lot of clusters to work with, so the averages aren't really stable.\n\n\n   ::: {.cell layout-align=\"center\"}\n   \n   ```{.r .cell-code}\n   predictions(\n     fit, \n     newdata = datagrid(TX = c(0, 1), \n                        cluster = unique), \n     by = \"TX\", \n     re_formula = NULL\n   )\n   ```\n   :::\n\n   ::: {.cell layout-align=\"center\"}\n   ::: {.cell-output .cell-output-stdout}\n   ```{.r .sourceCode}\n   ## # A tibble: 2 × 5\n   ##   type        TX predicted conf.low conf.high\n   ##   <chr>    <dbl>     <dbl>    <dbl>     <dbl>\n   ## 1 response     0      647.     502.      905.\n   ## 2 response     1      321.     250.      443.\n   ```\n   :::\n   :::\n\n\n   We can visualize the posteriors too:\n\n\n   ::: {.cell layout-align=\"center\"}\n   \n   ```{.r .cell-code}\n   marginal_preds <- predictions(\n     fit, \n     newdata = datagrid(TX = c(0, 1), \n                        cluster = unique), \n     by = \"TX\", \n     re_formula = NULL\n   ) %>% \n     posteriordraws()\n   ```\n   :::\n\n   ::: {.cell layout-align=\"center\"}\n   \n   ```{.r .cell-code}\n   p_marginal_preds <- marginal_preds %>% \n     ggplot(aes(x = draw, fill = factor(TX))) +\n     stat_halfeye() +\n     scale_fill_manual(values = colorspace::lighten(c(clrs[5], clrs[1]), 0.4)) +\n     scale_x_continuous(labels = label_dollar()) +\n     labs(x = \"Gambling losses\", y = \"Density\", fill = \"TX\",\n          title = \"Marginal population-level means\",\n          subtitle = \"Random effects averaged / marginalized / integrated\") +\n     coord_cartesian(xlim = c(100, 1500)) +\n     theme_nice()\n   p_marginal_preds\n   ```\n   \n   ::: {.cell-output-display}\n   ![](index_files/figure-html/plot-marginal-preds-1.png){fig-align='center' width=85%}\n   :::\n   :::\n\n\n5. **Integrate out random effects**: Instead of using the existing cluster intercepts, we can integrate out the random effects by generating predictions for a bunch of clusters (like 100), and then collapse those predictions into averages. This is similar to the intuition of brute force Monte Carlo integration in approach #3 earlier. *This takes a long time!* It results in the same estimates we found with the mathematical approaches in #1, #2, and #3 earlier.\n\n\n   ::: {.cell layout-align=\"center\"}\n   \n   ```{.r .cell-code}\n   predictions(fit, newdata = datagrid(TX = c(0, 1), cluster = c(-1:-100)),\n               allow_new_levels = TRUE,\n               sample_new_levels = \"gaussian\",\n               by = \"TX\")\n   ```\n   :::\n\n   ::: {.cell layout-align=\"center\"}\n   ::: {.cell-output .cell-output-stdout}\n   ```{.r .sourceCode}\n   ## # A tibble: 2 × 5\n   ##   type        TX predicted conf.low conf.high\n   ##   <chr>    <dbl>     <dbl>    <dbl>     <dbl>\n   ## 1 response     0      682.     461.     1168.\n   ## 2 response     1      340.     227.      577.\n   ```\n   :::\n   :::\n\n   ::: {.cell layout-align=\"center\"}\n   \n   ```{.r .cell-code}\n   marginal_preds_int <- predictions(\n     fit, \n     newdata = datagrid(TX = c(0, 1), \n                        cluster = c(-1:-100)),\n     re_formula = NULL,\n     allow_new_levels = TRUE,\n     sample_new_levels = \"gaussian\",\n     by = \"TX\"\n   ) %>% \n     posteriordraws()\n   ```\n   :::\n\n   ::: {.cell layout-align=\"center\"}\n   \n   ```{.r .cell-code}\n   p_marginal_preds_int <- marginal_preds_int %>% \n     ggplot(aes(x = draw, fill = factor(TX))) +\n     stat_halfeye() +\n     scale_fill_manual(values = colorspace::lighten(c(clrs[5], clrs[1]), 0.4)) +\n     scale_x_continuous(labels = label_dollar()) +\n     labs(x = \"Gambling losses\", y = \"Density\", fill = \"TX\",\n          title = \"Marginal population-level means\",\n          subtitle = \"Random effects integrated out\") +\n     coord_cartesian(xlim = c(100, 1500)) +\n     theme_nice()\n   p_marginal_preds_int\n   ```\n   \n   ::: {.cell-output-display}\n   ![](index_files/figure-html/plot-marginal-preds-int-1.png){fig-align='center' width=85%}\n   :::\n   :::\n\n\n\n## Population-level ATE\n\nThe average treatment effect (ATE) for a binary treatment is the difference between the two averages when $\\text{TX} = 1$ and $\\text{TX} = 0$, after somehow incorporating all the random cluster-specific offsets:\n\n$$\n\\textbf{E}(Y_{i_j} \\mid \\text{TX} = 1) - \\textbf{E}(Y_{i_j} \\mid \\text{TX} = 0)\n$$\n\nFor a lognormal family, it's this terrifying thing:\n\n$$\n\\begin{aligned}\n&\\textbf{E}(Y_{i_j} \\mid \\text{TX} = 1) - \\textbf{E}(Y_{i_j} \\mid \\text{TX} = 0) = \\\\\n&\\qquad \\int \\exp \\left(x + \\sigma_y^2 / 2 \\right) \\, f_{\\texttt{dnorm}} \\left(x, \\left(\\beta_0 + \\beta_1 \\right), \\sigma_0^2 \\right) \\,dx \\ - \\\\\n&\\qquad \\int \\exp \\left(x + \\sigma_y^2 / 2 \\right) \\, f_{\\texttt{dnorm}} \\left(x, \\beta_0, \\sigma_0^2 \\right) \\,dx\n\\end{aligned}\n$$\n\nThat looks scary, but really it's just the difference in the two estimates we found before: $\\textbf{E}(Y_{i_j} \\mid \\text{TX} = 1)$ and $\\textbf{E}(Y_{i_j} \\mid \\text{TX} = 0)$. We can use the same approaches from above and just subtract the two estimates, like this with the magical moment-generating function thing:\n\n2. Population-level ATE with **moment-generating function**:\n\n\n   ::: {.cell layout-align=\"center\"}\n   \n   ```{.r .cell-code}\n   exp(B_TXs[2] + (sigma_0^2 + sigma_y^2)/2) - \n     exp(B_TXs[1] + (sigma_0^2 + sigma_y^2)/2)\n   ##    1 \n   ## -349\n   ```\n   :::\n\n\nWe can do this with {marginaleffects} too, either by averaging / marginalizing / integrating across existing clusters (though again, this weirdly gives slightly different results) or by integrating out the random effects from a bunch of hypothetical clusters (which gives the same result as the more analytical / mathematical estimates):\n\n4. **Average / marginalize / integrate across existing random effects**:\n\n\n   ::: {.cell layout-align=\"center\"}\n   \n   ```{.r .cell-code}\n   # Marginal treatment effect (or global population level effect)\n   comparisons(\n     fit, \n     variables = \"TX\", \n     re_formula = NULL\n   ) %>% \n     tidy()\n   ```\n   :::\n\n   ::: {.cell layout-align=\"center\"}\n   ::: {.cell-output .cell-output-stdout}\n   ```{.r .sourceCode}\n   ## # A tibble: 1 × 6\n   ##   type     term  contrast estimate conf.low conf.high\n   ##   <chr>    <chr> <chr>       <dbl>    <dbl>     <dbl>\n   ## 1 response TX    1 - 0       -326.    -652.     -60.9\n   ```\n   :::\n   :::\n\n   ::: {.cell layout-align=\"center\"}\n   \n   ```{.r .cell-code}\n   marginal_ate <- comparisons(\n     fit, \n     variables = \"TX\", \n     re_formula = NULL\n   ) %>%\n     posteriordraws()\n   ```\n   :::\n\n   ::: {.cell layout-align=\"center\"}\n   \n   ```{.r .cell-code}\n   p_marginal_ate <- marginal_ate %>% \n     group_by(drawid) %>% \n     summarize(draw = mean(draw)) %>% \n     ggplot(aes(x = draw)) +\n     stat_halfeye(fill = colorspace::lighten(clrs[3], 0.4)) +\n     scale_x_continuous(labels = label_dollar(style_negative = \"minus\")) +\n     labs(x = \"(TX = 1) − (TX = 0)\", y = \"Density\", \n          title = \"Marginal population-level ATE\",\n          subtitle = \"Random effects averaged / marginalized / integrated\") +\n     coord_cartesian(xlim = c(-900, 300)) +\n     theme_nice()\n   p_marginal_ate\n   ```\n   \n   ::: {.cell-output-display}\n   ![](index_files/figure-html/plot-marginal-ate-1.png){fig-align='center' width=85%}\n   :::\n   :::\n\n\n5. **Integrate out random effects**\n\n\n   ::: {.cell layout-align=\"center\"}\n   \n   ```{.r .cell-code}\n   # This takes a *really* long time\n   comparisons(\n     fit, \n     variables = \"TX\", \n     newdata = datagrid(cluster = c(-1:-100)),\n     re_formula = NULL,\n     allow_new_levels = TRUE,\n     sample_new_levels = \"gaussian\"\n   ) %>% \n     tidy()\n   ```\n   :::\n\n   ::: {.cell layout-align=\"center\"}\n   ::: {.cell-output .cell-output-stdout}\n   ```{.r .sourceCode}\n   ## # A tibble: 1 × 6\n   ##   type     term  contrast estimate conf.low conf.high\n   ##   <chr>    <chr> <chr>       <dbl>    <dbl>     <dbl>\n   ## 1 response TX    1 - 0       -338.    -779.     -64.0\n   ```\n   :::\n   :::\n\n   ::: {.cell layout-align=\"center\"}\n   \n   ```{.r .cell-code}\n   marginal_ate_int <- comparisons(\n     fit, \n     variables = \"TX\", \n     newdata = datagrid(cluster = c(-1:-100)),\n     re_formula = NULL,\n     allow_new_levels = TRUE,\n     sample_new_levels = \"gaussian\"\n   ) %>% \n     posteriordraws()\n   ```\n   :::\n\n   ::: {.cell layout-align=\"center\"}\n   \n   ```{.r .cell-code}\n   p_marginal_ate_int <- marginal_ate_int %>% \n     group_by(drawid) %>% \n     summarize(draw = mean(draw)) %>% \n     ggplot(aes(x = draw)) +\n     stat_halfeye(fill = colorspace::lighten(clrs[3], 0.4)) +\n     scale_x_continuous(labels = label_dollar(style_negative = \"minus\")) +\n     labs(x = \"(TX = 1) − (TX = 0)\", y = \"Density\", \n          title = \"Marginal population-level ATE\",\n          subtitle = \"Random effects integrated out\") +\n     coord_cartesian(xlim = c(-900, 300)) +\n     theme_nice()\n   p_marginal_ate_int\n   ```\n   \n   ::: {.cell-output-display}\n   ![](index_files/figure-html/plot-marginal-ate-int-1.png){fig-align='center' width=85%}\n   :::\n   :::\n\n\n\n# Ratios and multiplicative effects\n\nFinally, we can work directly with the coefficients to get more slope-like effects, which is especially helpful when the coefficient of interest isn't for a binary variable. Typically with GLMs with log or logit links (like logit, Poisson, negative binomial, lognormal, etc.) we can exponentiate the coefficient to get it as an odds ratio or a multiplicative effect. That works here too:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nexp(B1)\n##  b_TX \n## 0.495\n```\n:::\n\n\nA one-unit increase in $\\text{TX}$ causes a 51% decrease (`exp(B1) - 1`) in the outcome. Great.\n\nThat's all fine here because the lognormal model doesn't have any weird nonlinearities or interactions, but in the case of logistic regression or anything with interaction terms, life gets more complicated, so it's better to work with `marginaleffects()` instead of exponentiating things by hand. If we use `type = \"link\"` we'll keep the results as logged odds, and then we can exponentiate them. All the other random effects options that we used before (`re_formula = NA`, `re_formula = NULL`, integrating effects out, and so on) work here too.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmarginaleffects(\n  fit, \n  variable = \"TX\", \n  type = \"link\",\n  newdata = datagrid(TX = 0)\n) %>% \n  mutate(across(c(estimate, conf.low, conf.high), ~exp(.))) %>% \n  select(rowid, term, estimate, conf.low, conf.high)\n## \n##  Term Estimate CI low CI high\n##    TX    0.496  0.279    0.88\n## \n## Columns: rowid, term, estimate, conf.low, conf.high\n```\n:::\n\n\nWe can visualize the odds-ratio-scale posterior for fun:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmarginaleffects(\n  fit, \n  variable = \"TX\", \n  type = \"link\",\n  newdata = datagrid(TX = 0)\n) %>% \n  posteriordraws() %>% \n  mutate(draw = exp(draw) - 1) %>% \n  ggplot(aes(x = draw)) +\n  stat_halfeye(fill = colorspace::darken(clrs[3], 0.4)) +\n  geom_vline(xintercept = 0) +\n  scale_x_continuous(labels = label_percent()) +\n  labs(x = \"Percent change in outcome\", y = \"Density\") +\n  theme_nice()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-41-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nIf we use `type = \"response\"`, we can get slopes at specific values of the coefficient (which is less helpful here, since $\\text{TX}$ can only be 0 or 1; but it's useful for continuous coefficients of interest).\n\n\n# Summary\n\nPhew, that was a lot. Here's a summary table to reference to help keep things straight.\n\n:::{.column-page-right}\n\n\n::: {.cell .column-body-outset layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nwrap_r <- function(x) glue::glue('<div class=\"sourceCode cell-code\"><pre class=\"sourceCode r\"><code class=\"sourceCode r\">{x}</code></pre></div>')\n\nconditional_out <- r\"{predictions(\n  fit, \n  newdata = datagrid(TX = c(0, 1)), \n  by = \"TX\", \n  re_formula = NA\n)}\"\n\nconditional_ate <- r\"{comparisons(\n  fit, \n  variables = \"TX\",\n  re_formula = NA\n)}\"\n\nmarginal_out <- r\"{predictions(\n  fit, \n  newdata = datagrid(TX = c(0, 1), \n                     cluster = unique), \n  by = \"TX\", \n  re_formula = NULL\n)}\"\n\nmarginal_out_int <- r\"{predictions(\n  fit, \n  newdata = datagrid(TX = c(0, 1), \n                     cluster = c(-1:-100)),\n  re_formula = NULL,\n  allow_new_levels = TRUE,\n  sample_new_levels = \"gaussian\",\n  by = \"TX\"\n)}\"\n\nmarginal_ate <- r\"{comparisons(\n  fit, \n  variables = \"TX\", \n  re_formula = NULL\n) %>% \n  tidy()\n}\"\n\nmarginal_ate_int <- r\"{comparisons(\n  fit, \n  variables = \"TX\", \n  newdata = datagrid(cluster = c(-1:-100)),\n  re_formula = NULL,\n  allow_new_levels = TRUE,\n  sample_new_levels = \"gaussian\"\n) %>% \n  tidy()\n}\"\n\ntribble(\n  ~Effect, ~Formula, ~`{marginaleffects} code`,\n  \"Average outcomes in typical group\", \"\\\\(\\\\textbf{E}(Y_{i_j} \\\\mid b_{0_j} = 0, \\\\text{TX} = \\\\{0, 1\\\\})\\\\)\", wrap_r(conditional_out),\n  \"ATE in typical group\", \"\\\\(\\\\textbf{E}(Y_{i_j} \\\\mid b_{0_j} = 0, \\\\text{TX} = 1) -\\\\)<br> \\\\(\\\\quad\\\\textbf{E}(Y_{i_j} \\\\mid b_{0_j} = 0, \\\\text{TX} = 0)\\\\)\", wrap_r(conditional_ate),\n  \"Average population-level outcomes (marginalized)\", \"\\\\(\\\\textbf{E}(Y_{i_j} \\\\mid \\\\text{TX} = \\\\{0, 1\\\\})\\\\)\", wrap_r(marginal_out),\n  \"Average population-level outcomes (integrated out)\", \"\\\\(\\\\textbf{E}(Y_{i_j} \\\\mid \\\\text{TX} = \\\\{0, 1\\\\})\\\\)\", wrap_r(marginal_out_int),\n  \"Population-level ATE (marginalized)\", \"\\\\(\\\\textbf{E}(Y_{i_j} \\\\mid \\\\text{TX} = 1) -\\\\)<br> \\\\(\\\\quad\\\\textbf{E}(Y_{i_j} \\\\mid \\\\text{TX} = 0)\\\\)\", wrap_r(marginal_ate),\n  \"Population-level ATE (integrated out)\", \"\\\\(\\\\textbf{E}(Y_{i_j} \\\\mid \\\\text{TX} = 1) -\\\\)<br> \\\\(\\\\quad\\\\textbf{E}(Y_{i_j} \\\\mid \\\\text{TX} = 0)\\\\)\", wrap_r(marginal_ate_int)\n) %>% \n  kbl(escape = FALSE, align = c(\"l\", \"l\", \"l\")) %>% \n  kable_styling(htmltable_class = \"table table-sm\") %>% \n  pack_rows(index = c(\"Conditional effects\" = 2, \"Marginal effects\" = 4)) %>% \n  column_spec(1, width = \"25%\") |> \n  column_spec(2, width = \"35%\") |> \n  column_spec(3, width = \"40%\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\" table table-sm\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Effect </th>\n   <th style=\"text-align:left;\"> Formula </th>\n   <th style=\"text-align:left;\"> {marginaleffects} code </th>\n  </tr>\n </thead>\n<tbody>\n  <tr grouplength=\"2\"><td colspan=\"3\" style=\"border-bottom: 1px solid;\"><strong>Conditional effects</strong></td></tr>\n<tr>\n   <td style=\"text-align:left;padding-left: 2em;width: 25%; \" indentlevel=\"1\"> Average outcomes in typical group </td>\n   <td style=\"text-align:left;width: 35%; \"> \\(\\textbf{E}(Y_{i_j} \\mid b_{0_j} = 0, \\text{TX} = \\{0, 1\\})\\) </td>\n   <td style=\"text-align:left;width: 40%; \"> <div class=\"sourceCode cell-code\"><pre class=\"sourceCode r\"><code class=\"sourceCode r\">predictions(\n  fit, \n  newdata = datagrid(TX = c(0, 1)), \n  by = \"TX\", \n  re_formula = NA\n)</code></pre></div> </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;padding-left: 2em;width: 25%; \" indentlevel=\"1\"> ATE in typical group </td>\n   <td style=\"text-align:left;width: 35%; \"> \\(\\textbf{E}(Y_{i_j} \\mid b_{0_j} = 0, \\text{TX} = 1) -\\)<br> \\(\\quad\\textbf{E}(Y_{i_j} \\mid b_{0_j} = 0, \\text{TX} = 0)\\) </td>\n   <td style=\"text-align:left;width: 40%; \"> <div class=\"sourceCode cell-code\"><pre class=\"sourceCode r\"><code class=\"sourceCode r\">comparisons(\n  fit, \n  variables = \"TX\",\n  re_formula = NA\n)</code></pre></div> </td>\n  </tr>\n  <tr grouplength=\"4\"><td colspan=\"3\" style=\"border-bottom: 1px solid;\"><strong>Marginal effects</strong></td></tr>\n<tr>\n   <td style=\"text-align:left;padding-left: 2em;width: 25%; \" indentlevel=\"1\"> Average population-level outcomes (marginalized) </td>\n   <td style=\"text-align:left;width: 35%; \"> \\(\\textbf{E}(Y_{i_j} \\mid \\text{TX} = \\{0, 1\\})\\) </td>\n   <td style=\"text-align:left;width: 40%; \"> <div class=\"sourceCode cell-code\"><pre class=\"sourceCode r\"><code class=\"sourceCode r\">predictions(\n  fit, \n  newdata = datagrid(TX = c(0, 1), \n                     cluster = unique), \n  by = \"TX\", \n  re_formula = NULL\n)</code></pre></div> </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;padding-left: 2em;width: 25%; \" indentlevel=\"1\"> Average population-level outcomes (integrated out) </td>\n   <td style=\"text-align:left;width: 35%; \"> \\(\\textbf{E}(Y_{i_j} \\mid \\text{TX} = \\{0, 1\\})\\) </td>\n   <td style=\"text-align:left;width: 40%; \"> <div class=\"sourceCode cell-code\"><pre class=\"sourceCode r\"><code class=\"sourceCode r\">predictions(\n  fit, \n  newdata = datagrid(TX = c(0, 1), \n                     cluster = c(-1:-100)),\n  re_formula = NULL,\n  allow_new_levels = TRUE,\n  sample_new_levels = \"gaussian\",\n  by = \"TX\"\n)</code></pre></div> </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;padding-left: 2em;width: 25%; \" indentlevel=\"1\"> Population-level ATE (marginalized) </td>\n   <td style=\"text-align:left;width: 35%; \"> \\(\\textbf{E}(Y_{i_j} \\mid \\text{TX} = 1) -\\)<br> \\(\\quad\\textbf{E}(Y_{i_j} \\mid \\text{TX} = 0)\\) </td>\n   <td style=\"text-align:left;width: 40%; \"> <div class=\"sourceCode cell-code\"><pre class=\"sourceCode r\"><code class=\"sourceCode r\">comparisons(\n  fit, \n  variables = \"TX\", \n  re_formula = NULL\n) %&gt;% \n  tidy()\n</code></pre></div> </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;padding-left: 2em;width: 25%; \" indentlevel=\"1\"> Population-level ATE (integrated out) </td>\n   <td style=\"text-align:left;width: 35%; \"> \\(\\textbf{E}(Y_{i_j} \\mid \\text{TX} = 1) -\\)<br> \\(\\quad\\textbf{E}(Y_{i_j} \\mid \\text{TX} = 0)\\) </td>\n   <td style=\"text-align:left;width: 40%; \"> <div class=\"sourceCode cell-code\"><pre class=\"sourceCode r\"><code class=\"sourceCode r\">comparisons(\n  fit, \n  variables = \"TX\", \n  newdata = datagrid(cluster = c(-1:-100)),\n  re_formula = NULL,\n  allow_new_levels = TRUE,\n  sample_new_levels = \"gaussian\"\n) %&gt;% \n  tidy()\n</code></pre></div> </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n:::\n\nAnd here are all the posteriors all together, for easier comparison:\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n((p_conditional_preds + coord_cartesian(xlim = c(0, 1200))) | p_conditional_ate) /\n  ((p_marginal_preds + coord_cartesian(xlim = c(0, 1200))) | p_marginal_ate) /\n  ((p_marginal_preds_int + coord_cartesian(xlim = c(0, 1200))) | p_marginal_ate_int)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-everything-1.png){fig-align='center' width=85%}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../../../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../../../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
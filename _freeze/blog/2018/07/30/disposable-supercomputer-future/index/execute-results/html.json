{
  "hash": "2e99a85b5f08ac2775a63f5bc5ae794f",
  "result": {
    "markdown": "---\ntitle: Create a cheap, disposable supercomputer with R, DigitalOcean, and future\ndate: 2018-07-30\ndescription: Use the future R package to run computationally intensive R commands on a cluster of remote computers\ncaategories: \n  - r\n  - docker\n  - tidyverse\n  - future\n  - digitalocean\n  - docker\n---\n\n\n\n\n\n*tl;dr*: [Skip to complete example](#full-example)\n\n---\n\nIn one of my [current research projects](https://github.com/andrewheiss/donors-ngo-restrictions), I use Bayesian modeling (with [Stan](http://mc-stan.org/) and [rstanarm](http://mc-stan.org/rstanarm/articles/rstanarm.html)) and multiple imputation (with [Amelia](https://gking.harvard.edu/Amelia)) to measure how international aid agencies change their funding allocations to countries that impose legal restrictions on NGOs. It's a fascinating topic and I'm using exciting cutting edge research methods to do it.\n\nHowever, these cutting edge research methods are *really* computationally intensive. On my laptop, using all four CPU cores, it takes ≈45 minutes to run one set of models (see [`h1.barriers.total <- mods.h1.next_year.raw.bayes.nested` here](https://github.com/andrewheiss/donors-ngo-restrictions/blob/cb93d175e29b88502640e70b48f5a899dbdba1c4/Data/run_bayes_remote.R#L95), for instance), so with all the different model specifications and robustness checks, it takes *hours* to run the complete analysis for this project. It's awful and I hate rerunning it.\n\nIn the past, I created several [DigitalOcean](https://www.digitalocean.com/) droplets (what they call virtual private servers), installed [RStudio Server](https://www.rstudio.com/products/rstudio/download-server/) on each, uploaded my data to each instance, and ran separate models on each. This reduced computation time, since I could have like 5 computers all running different parts of the analysis, but it required a ton of manual work and oversight. Computers are really good at automating stuff,  and tools like [Slurm](https://slurm.schedmd.com/) and [Hadoop](http://hadoop.apache.org/) and [Kubernetes](https://kubernetes.io/) all allow you to orchestrate computing across clusters of machines. However, because these tools are powerful, they're also really complicated and require a lot of additional configuration, and the tradeoff between teaching myself Kubernetes vs. just doing it all manually wasn't that favorable.\n\nSurely, [I mused on Twitter last week](https://twitter.com/andrewheiss/status/1022937013022945280?s=21), there's a better easier way to manage all this computing.\n\nThere is! And it's surprisingly easy with the [**future**](https://github.com/HenrikBengtsson/future) package in R!\n\nYou can create your own disposable supercomputer with remote clusters of computers in just a few intuitive steps. \n\n1. Create remote computers that have the correct R environment and packages set up already\n2. Use `future::plan()` to point R to those computers\n3. Run R commands on those computers with `future::future_lapply()` or `furrr::future_map()`\n4. Throw away the remote computers when you're done\n5. That's all!\n\n[A fully worked out, tl;dr example is at the end of this post.](#full-example)\n\n\n## 0. Super quick introduction to future\n\nThe key to all of this working correctly is the [**future**](https://github.com/HenrikBengtsson/future) package in R. **future** allows you to evaluate R commands and expressions in separate processes.\n\nFor instance, ordinarily, when you run this command, `x` gets assigned a value immediately:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx <- {\n  cat(\"Something really computationally intensive\\n\")\n  10\n}\n#> Something really computationally intensive\n\nx\n#> [1] 10\n```\n:::\n\n\nThe stuff in `cat()` gets printed immediately and the value of 10 is assigned to `x` immediately as well. This is all well and good, but if the command is computationally intensive, you have to wait until it's done before doing anything else in R.\n\n**future** includes a special assignment command `%<-%` that delays evaluation until `x` is called.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(future)\n\nx %<-% {\n    cat(\"Something really computationally intensive\\n\")\n    10\n}\n\nx\n#> Something really computationally intensive\n#> [1] 10\n```\n:::\n\n\nNotice how `cat()` isn't run until `x` is run. That's because the whole expression isn't actually run yet—it isn't evaluated until `x` is called.\n\nThe magic of **future** is that this deferred evaluation *can automatically happen anywhere else*. You specify where evaluation happens with `future::plan()`. For instance, if you want `x` to be handled on multiple CPUs on your local computer, you'd do this:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(future)\nplan(multiprocess)\n\nx %<-% {\n  cat(\"Something really computationally intensive\\n\")\n  10\n}\n\nx\n#> Something really computationally intensive\n#> [1] 10\n```\n:::\n\n\nR now uses multiple cores to create `x`.\n\nOr, if you want `x` to be processed on a cluster of three remote computers, you'd do this:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(future)\n\nips <- c(\"192.168.1.1\", \"192.168.1.2\", \"192.168.1.3\")\nplan(remote, workers = ips)\n\nx %<-% {\n    cat(\"Something really computationally intensive\\n\")\n    10\n}\n\nx\n#> Something really computationally intensive\n#> [1] 10\n```\n:::\n\n\n`x` is now created across three computers automatically. \n\n**future** has functions like [`future.apply::future_lapply()`](https://github.com/HenrikBengtsson/future.apply) that allow you to apply functions to lists, just like `lapply()` and friends in base R, and the [**furrr** package](https://github.com/DavisVaughan/furrr) has futurized functions like `future_map()` that are equivalent to `map()` and friends in **purrr**. The `future_*` versions of these functions will automatically take advantage of whatever you've specified in `plan()`. If you use `plan(multiprocess)`, `future_map()` will automatically send chunks of computations to each of the CPU cores; if you use `plan(remote)`, `future_map()` will automatically send chunks of computations to each of the remote computers.\n\nThis is seriously magic and incredible. The backend you specify with `plan()` *doesn't matter* and you can change it later to anything you want *without having to change your code*.\n\n\n## 1. Create remote computers that have the correct R environment and packages set up already\n\n### 1.1. Create a remote computer (or two or three)\n\nFirst, you need a remote computer. I prefer to use DigitalOcean, mostly because I already use it for my personal web hosting and other personal projects, and because I find it way more intuitive and easy to use than [Amazon's EC2](https://aws.amazon.com/ec2/) (which, like Kubernetes, et al. is incredibly powerful, but incredibly complicated). But you can also do all of this with AWS, [Linode](https://www.linode.com/), [Google Cloud Platform](https://cloud.google.com/), or any other VPS service ([here's how to do it with AWS](https://gist.github.com/DavisVaughan/ef544e6ef2228920e7e7100c48def93e) and with [Google Cloud](https://cloudyr.github.io/googleComputeEngineR/articles/massive-parallel.html)). The magic of **future** is that it doesn't matter what backend you use—the package takes care of everything for you.\n\nHere's what to do with DigitalOcean:\n\n1. Create a DigitalOcean account ([get $10 for free with this link](https://m.do.co/c/cec0de11762e))\n\n2. Create an SSH key pair between your DigitalOcean account and your computer. [Follow the instructions here](https://www.digitalocean.com/docs/droplets/how-to/add-ssh-keys/) to create SSH keys and then upload the public keys to your account. Setting up SSH keys like this lets you securely access your remote computer without a password, which is nice when running remote computations from R.\n\n3. In your DigitalOcean account, create a new Droplet. For simplicity's sake, you can use the \"One-click apps\" tab to create a computer that has [Docker](https://www.docker.com/) pre-installed. \n\n\n   ::: {.cell layout-align=\"center\"}\n   \n   :::\n\n\n4. Choose how big you want the droplet to be. For now, we'll just make a small $5/month VPS, but you can get as fancy as you want in real life.\n\n5. Check the box near the bottom to add your SSH key to the VPS automatically.\n\n\n   ::: {.cell layout-align=\"center\"}\n   \n   :::\n\n\n6. All done! Take note of the IP address. You can connect to the machine now with a terminal with `ssh root@IPADDRESS` if you want, but you don't need to.\n\nYou can automate all this with the [**analogsea** package](https://github.com/sckott/analogsea) in R. Create an API key in your DigitalOcean account:\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\nThen edit `~/.Rprofile` (or create a new file there, if needed) and add this line:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nSys.setenv(DO_PAT = \"KEY_GOES_HERE\")\n```\n:::\n\n\nNow, you can use R to create DigitalOcean droplets, like so:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(analogsea)\n\n# droplet_create() makes a generic Linux VPS\nremote_computer <- droplet_create(region = \"sfo2\", size = \"1gb\")\n\n# Or...\n# docklet_create() makes a Linux VPS with Docker pre-installed\nremote_computer <- docklet_create(region = \"sfo2\", size = \"1gb\")\n```\n:::\n\n\n### 1.2. Make sure the remote computer has the correct R environment and packages\n\nThe computers you specify in `plan(remote, workers = \"blah\")` need to (1) have R installed, and (2) have all the packages installed that are needed for the computation. You can manually install R and all the needed packages, but that can take a long time and it's tedious. If you need a cluster of 6 computers, you don't want to take an hour to install R on each of them (and wait for all the packages to compile).\n\nThe easiest way to get a ready-to-go R environment is to use [Docker](https://www.docker.com/). I won't cover the details of Docker here (since [I've done that already](https://www.andrewheiss.com/blog/2017/04/27/super-basic-practical-guide-to-docker-and-rstudio/))—essentially, Docker lets you install pre-configured virtual computers instantly. For example, [`rocker/rbase`](https://hub.docker.com/r/rocker/r-base/) is a Linux machine with R preinstalled, and [`rocker/tidyverse`](https://hub.docker.com/r/rocker/tidyverse/) is a Linux machine with R + RStudio server + tidyverse pre-installed. You can also create project-specific environments like [this one for my donors-NGOs project](https://hub.docker.com/r/andrewheiss/docker-donors-ngo-restrictions/), and you can use R packages like [**containerit**](https://github.com/o2r-project/containerit) to automatically create a `Dockerfile` out of your current R environment. It's probably best to make your own custom Docker image for your own projects if you use any packages beyond what's in the default `rbase` or `tidyverse` images.\n\nTo get a Docker image onto your remote computer, log into the computer with `ssh root@IPADDRESS` in your terminal and run `docker pull rocker/tidyverse`. \n\nIf you're using **analogsea**, run this from R:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndroplet(remote_computer$id) %>% docklet_pull(\"rocker/tidyverse\")\n```\n:::\n\n\nYou can do this to create as many remote computers as you want.\n\nThe first time you run `docklet_pull()` on a droplet, Docker will download hundreds of megabytes of container files. This is fine for one computer, but if you're creating a cluster of multiple computers, you might not want to redownload everything every time on each computer (to avoid extra bandwidth charges, for example). Instead of pulling a fresh Docker image on each computer, you can take a snapshot of the first remote droplet and then make new droplets based on the snapshot:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Create new droplet with rocker/tidyverse\nremote_computer <- docklet_create(region = \"sfo2\", size = \"1gb\")\ndroplet(remote_computer$id) %>% docklet_pull(\"rocker/tidyverse\")\n\n# Create snapshot\ndroplet(remote_computer$id) %>% \n  droplet_power_off() %>% \n  droplet_snapshot(name = \"tidyverse_ready\") %>% \n  droplet_power_on()\n\n# Create a new droplet based on this snapshot. This new computer will already\n# have rocker/tidyverse on it\n#\n# You can see a list of available snapshots and get the name/id with\n# images(private = TRUE)\nremote_computer2 <- droplet_create(image = \"12345678\", \n                                   region = \"sfo2\", size = \"1gb\")\n\n# This won't take long because it already has rocker/tidyverse\n# You should get this message:\n#   Status: Image is up to date for rocker/tidyverse:latest\ndroplet(remote_computer$id) %>% docklet_pull(\"rocker/tidyverse\")\n```\n:::\n\n\nJust make sure you delete the snapshots when you're done—they cost $0.05 per GB per month.\n\n\n## 2. Use `future::plan()` to point R to those computers\n\nWe can now point R to this remote computer (or remote computers) and have **future** automatically use the Docker-installed R.\n\nIf we didn't use Docker and instead installed R on the remote machine itself, all we'd need to do is run this in R:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplan(remote, workers = \"IP ADDRESS HERE\")\n```\n:::\n\n\nHowever, because R lives inside a Docker image, we need to do a tiny bit of extra configuration on the local computer—we have to tell the remote computer how to turn on and access the R Docker image. We do this by defining a cluster. Here's a heavily commented example of how to do that:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Public IP for droplet(s); this can also be a vector of IP addresses\nip <- IP_ADDRESS_HERE\n\n# Path to private SSH key that matches key uploaded to DigitalOcean\nssh_private_key_file <- \"/Users/andrew/.ssh/id_rsa\"\n\n# Connect and create a cluster\ncl <- makeClusterPSOCK(\n  ip,\n  \n  # User name; DigitalOcean droplets use root by default\n  user = \"root\",\n  \n  # Use private SSH key registered with DigitalOcean\n  rshopts = c(\n    \"-o\", \"StrictHostKeyChecking=no\",\n    \"-o\", \"IdentitiesOnly=yes\",\n    \"-i\", ssh_private_key_file\n  ),\n  \n  # Command to run on each remote machine\n  # The script loads the tidyverse Docker image\n  # --net=host allows it to communicate back to this computer\n  rscript = c(\"sudo\", \"docker\", \"run\", \"--net=host\", \n              \"rocker/tidyverse\", \"Rscript\"),\n  \n  # These are additional commands that are run on the remote machine. \n  # At minimum, the remote machine needs the future library to work—installing furrr also installs future.\n  rscript_args = c(\n    # Create directory for package installation\n    \"-e\", shQuote(\"local({p <- Sys.getenv('R_LIBS_USER'); dir.create(p, recursive = TRUE, showWarnings = FALSE); .libPaths(p)})\"),\n    # Install furrr and future\n    \"-e\", shQuote(\"if (!requireNamespace('furrr', quietly = TRUE)) install.packages('furrr')\")\n  ),\n  \n  # Actually run this stuff. Set to TRUE if you don't want it to run remotely.\n  dryrun = FALSE\n)\n```\n:::\n\n\nWith this cluster defined, we can now use it in `future::plan()`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplan(cluster, workers = cl)\n```\n:::\n\n\nAnd that's it! **future** is now ready to run commands on the remote computer.\n\n\n## 3. Run R commands on those computers with `future::future_lapply()` or `furrr::future_map()`\n\nNow we can run **future**-based commands on the remote computer with `%<-%`. First, let's check the remote computer's name:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Verify that commands run remotely by looking at the name of the remote\n# Create future expression; this doesn't run remotely yet\nremote_name %<-% {\n  Sys.info()[[\"nodename\"]]\n} \n\n# Run remote expression and see that it's running inside Docker, not locally\nremote_name\n#> [1] \"docker-s-1vcpu-2gb-sfo2-01\"\n```\n:::\n\n\nHow many CPUs does it have?\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# See how many CPU cores the remote machine has\nn_cpus %<-% {parallel::detectCores()} \nn_cpus\n#> [1] 1\n# Just one, since this is a small machine\n```\n:::\n\n\nWe can now outsource any command we want to the remote computer. We don't even have to transfer data manually to the remote—**future** takes care of all of that automatically:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Do stuff with data locally\ntop_5_worlds <- starwars %>% \n  filter(!is.na(homeworld)) %>% \n  count(homeworld, sort = TRUE) %>% \n  slice(1:5) %>% \n  mutate(homeworld = fct_inorder(homeworld, ordered = TRUE))\n\n# Create plot remotely, just for fun\nhomeworld_plot %<-% { \n  ggplot(top_5_worlds, aes(x = homeworld, y = n)) +\n    geom_bar(stat = \"identity\") + \n    labs(x = \"Homeworld\", y = \"Count\", \n         title = \"Most Star Wars characters are from Naboo and Tatooine\",\n         subtitle = \"It really is a Skywalker/Amidala epic\")\n}\n\n# Run the command remotely and show plot locally\n# Note how we didn't have to load any data on the remote machine. future takes\n# care of all of that for us!\nhomeworld_plot\n```\n:::\n\n\n<img src=\"star-wars-homeworlds.png\" width=\"75%\" style=\"display: block; margin: auto;\" alt=\"Top homeworlds in Star Wars\" />\n\nIn addition to `%<-%`, we can use functions like `furrr::future_map()` to run functions across a vector of values. Because we're only running one remote computer, all these calculations happen on that remote image. If we used more, **future** would automatically dispatch different chunks of this code across different computers and then reassemble them locally. Anything you can do with **furrr** or **future** can now be done remotely.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Multiply the numbers 1-5 by 10000 random numbers\n# All these calculations happen remotely!\nfuture_map(1:5, ~ rnorm(10000) * .x)\n#> [[1]]\n#>     [1] -6.249602e-01  1.949156e+00  2.205669e+00  4.525842e-01\n#> ...\n```\n:::\n\n\nYou can even nest **future** plans by [specifying them in a list](https://cran.r-project.org/web/packages/future/vignettes/future-3-topologies.html), and the package will take care of the different nested layers automatically. You can also place `plan(list(...))` in a file named `.future.R`, which **future** will source automatically when it is loaded. This allows you to use different plans on different computers without ever changing your code.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplan(list( \n  tweak(cluster, workers = cl),  # Use a cluster of computers locally \n  multiprocess  # Use all the CPUs on remote machines \n)) \n\ncomplicated_remote_stuff %<-% {  \n  # Do complicated stuff on all the remote CPUs with future or furrr functions\n  future_map(1:5, ~ rnorm(10000) * .x)\n} \n\ncomplicated_remote_stuff\n#> [[1]]\n#>     [1] -4.746093e-02  1.874897e+00  1.679342e+00 -5.775777e-01\n#> ...\n```\n:::\n\n\n\n## 4. Throw away the remote computers when you're done\n\nBecause we've used Docker-based R installations, spinning up new droplets is trivial—you don't have to manually install R and all the packages you need by hand. All these remote images are completely disposable. \n\nOnce you're done running stuff remotely, you can delete the droplets and save money. Either delete them through the DigitalOcean dashboard, or use **analogsea**:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndroplet_destroy(remote_computer$id)\n```\n:::\n\n\n\n## 5. That's all!\n\nPhew. That's all! This is a lot easier than figuring out how to orchestrate Docker images with Kubernetes or figuring out how to create Hadoop clusters. **future** takes care of all the hard work behind the scenes and this all Just Works™.\n\n\n## Full example\n\nHere's a real-life example of using Stan to estimate a bunch of models on a cluster of two 4-CPU, 8 GB RAM machines. It uses the [`andrewheiss/docker-donors-ngo-restrictions`](https://hub.docker.com/r/andrewheiss/docker-donors-ngo-restrictions/) Docker image because it already has Stan and family pre-installed. Each machine costs $0.06 per hour to run, so it's essentially a cheap, fast, remote, and disposable supercomputer.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load libraries ----------------------------------------------------------\n\nlibrary(tidyverse)\nlibrary(analogsea)\nlibrary(broom)\nlibrary(rstanarm)\nlibrary(gapminder)\nlibrary(tictoc)\nlibrary(ggstance)\n\n# Path to private SSH key that matches key on DigitalOcean\nssh_private_key_file <- \"/Users/andrew/.ssh/id_rsa\"\n\n\n# Set up remote machines --------------------------------------------------\n\n# Create two new droplets with Docker pre-installed\n# Here I'm using \"s-4vcpu-8gb\", which has 4 CPUs and 8 GB of RAM.\n# Run analogsea::sizes() to see all the available sizes\ndroplet1 <- docklet_create(region = \"sfo2\", size = \"s-4vcpu-8gb\")\ndroplet2 <- docklet_create(region = \"sfo2\", size = \"s-4vcpu-8gb\")\n\n# Pull the docker image with the environment for this project\n#\n# Here I'm using andrewheiss/docker-donors-ngo-restrictions because it already\n# has rstan and friends installed; none of the rocker R images do that\n#\n# NB: Wait for a minute before running this so that Docker is ready to\n# run on the remote machines\ndroplet(droplet1$id) %>% \n  docklet_pull(\"andrewheiss/docker-donors-ngo-restrictions\")\n\ndroplet(droplet2$id) %>% \n  docklet_pull(\"andrewheiss/docker-donors-ngo-restrictions\")\n\n# Get IP addresses\nip1 <- droplet(droplet1$id)$networks$v4[[1]]$ip_address\nip2 <- droplet(droplet2$id)$networks$v4[[1]]$ip_address\n\nips <- c(ip1, ip2)\n\n\n# Make remote cluster -----------------------------------------------------\n\n# Command to run on each remote machine\n# The script loads the docker-donors-ngo-restrictions Docker image\n# --net=host allows it to communicate back to this computer\nrscript <- c(\"sudo\", \"docker\", \"run\", \"--net=host\", \n             \"andrewheiss/docker-donors-ngo-restrictions\", \"Rscript\")\n\n# Connect and create a cluster\ncl <- makeClusterPSOCK(\n  ips,\n  \n  # User name; DO droplets use root by default\n  user = \"root\",\n  \n  # Use private SSH key registered with DO\n  rshopts = c(\n    \"-o\", \"StrictHostKeyChecking=no\",\n    \"-o\", \"IdentitiesOnly=yes\",\n    \"-i\", ssh_private_key_file\n  ),\n  \n  rscript = rscript,\n  \n  # Things to run each time the remote instance starts\n  rscript_args = c(\n    # Set up .libPaths() for the root user and install future/purrr/furrr packages\n    # Technically future and furrr are already installed on \n    # andrewheiss/docker-donors-ngo-restrictions, so these won't do anything\n    \"-e\", shQuote(\"local({p <- Sys.getenv('R_LIBS_USER'); dir.create(p, recursive = TRUE, showWarnings = FALSE); .libPaths(p)})\"),\n    \"-e\", shQuote(\"if (!requireNamespace('furrr', quietly = TRUE)) install.packages('furrr')\"),\n    # Make sure the remote computer uses all CPU cores with Stan\n    \"-e\", shQuote(\"options(mc.cores = parallel::detectCores())\")\n  ),\n  \n  dryrun = FALSE\n)\n\n# Use the cluster of computers as the backend for future and furrr functions\nplan(cluster, workers = cl)\n\n# We'll use gapminder data to estimate the relationship between health and \n# wealth in each continent using a Bayesian model\n\n# Process and manipulate data locally\n# Nest continent-based data frames into one larger data frame\ngapminder_to_model <- gapminder %>% \n  group_by(continent) %>% \n  nest() %>% \n  # Not enough observations here, so ignore it\n  filter(continent != \"Oceania\")\ngapminder_to_model\n#> # A tibble: 4 x 2\n#>   continent data              \n#>   <fct>     <list>            \n#> 1 Asia      <tibble [396 × 5]>\n#> 2 Europe    <tibble [360 × 5]>\n#> 3 Africa    <tibble [624 × 5]>\n#> 4 Americas  <tibble [300 × 5]> \n\n# Fit a Bayesian model with naive normal priors on the coefficients and\n# intercept on each of the continents. In real life, you'd want to use less\n# naive priors and rescale your data, but this is just an example.\nmodel_to_run <- function(df) {\n  model_stan <- stan_glm(lifeExp ~ gdpPercap + country, \n                         data = df, family = gaussian(),\n                         prior = normal(), prior_intercept = normal(),\n                         chains = 4, iter = 2000, warmup = 1000, seed = 1234)\n  return(model_stan)\n}\n\n# Use future_map to outsource each of the continent-based models to a different\n# remote computer, where it will be run with all 4 remote cores\ntic()\ngapminder_models <- gapminder_to_model %>% \n  mutate(model = data %>% future_map(~ model_to_run(.x)))\ntoc()\n#> 27.786 sec elapsed\n\n# That's so fast!\n\n# It worked!\ngapminder_models\n#> # A tibble: 4 x 3\n#>   continent data               model        \n#>   <fct>     <list>             <list>       \n#> 1 Asia      <tibble [396 × 5]> <S3: stanreg>\n#> 2 Europe    <tibble [360 × 5]> <S3: stanreg>\n#> 3 Africa    <tibble [624 × 5]> <S3: stanreg>\n#> 4 Americas  <tibble [300 × 5]> <S3: stanreg>\n\n\n# Do stuff with the models ------------------------------------------------\n\n# Extract the gdpPercap coefficient from the rstanarm models\ngapminder_models_to_plot <- gapminder_models %>% \n  mutate(tidied = model %>% map(~ tidy(.x, intervals = TRUE, prob = 0.9))) %>% \n  unnest(tidied) %>% \n  filter(term == \"gdpPercap\")\n\n# Plot the coefficients\nggplot(gapminder_models_to_plot, aes(x = estimate, y = continent)) +\n  geom_vline(xintercept = 0) +\n  geom_pointrangeh(aes(xmin = lower, xmax = upper, color = continent), size = 1) +\n  labs(x = \"Coefficient estimate (log GDP per capita)\", y = NULL,\n       caption = \"Bars show 90% credible intervals\") +\n  scale_color_viridis_d(begin = 0, end = 0.9, name = NULL) +\n  theme_grey() + theme(legend.position = \"bottom\")\n```\n:::\n\n\n<img src=\"model-coefs.png\" width=\"75%\" style=\"display: block; margin: auto;\" alt=\"Model coefficients by continent\" />\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Delete droplets ---------------------------------------------------------\n\ndroplet_delete(droplet1$id)\ndroplet_delete(droplet2$id)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
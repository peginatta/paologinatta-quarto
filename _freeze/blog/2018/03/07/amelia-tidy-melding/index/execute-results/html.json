{
  "hash": "a5964a7a682a1c453e62698407a0cf23",
  "result": {
    "markdown": "---\ntitle: Meld regression output from multiple imputations with tidyverse\ndate: 2018-03-07\ndescription: Use tidyverse functions to correctly meld and pool multiply imputed model output.\ncategories: \n  - r\n  - imputation\n  - tidyverse\n---\n\n\n\n\n<span class=\"small\">([See this notebook on GitHub](https://github.com/andrewheiss/amelia-tidy-melding))</span>\n\n---\n\nMissing data can significantly influence the results of normal regression models, since the default in R and most other statistical packages is to throw away any rows with missing variables. To avoid unnecessarily throwing out data, it's helpful to impute missing values. One of the best ways to do this is to build a separate regression model to make predictions that fill in the gaps in data. This isn't always accurate, so it's best to make many iterations of predictions (in imputation parlance, $m$ is the number of imputations done to a dataset). After making $m$ datasets, you can use this data by (1) running statistical tests on each imputation individually and then (2) pooling those results into a single number. The [excellent Amelia vignette](https://cran.r-project.org/web/packages/Amelia/vignettes/amelia.pdf) details the theory and mechanics of how to use multiple imputation, and it's a fantastic resource.\n\nThere are several packages for dealing with missing data in R, including [`mi`](https://cran.r-project.org/package=mi), [`mice`](https://cran.r-project.org/package=mice), and [`Amelia`](https://cran.r-project.org/package=Amelia), and Thomas Leeper has [a short overview of how to use all three](http://thomasleeper.com/Rcourse/Tutorials/mi.html). I'm partial to [Amelia](https://gking.harvard.edu/amelia), since it's designed to work well with time series-cross sectional data and can deal with complicated features like country-year observations. \n\nBecause Amelia is written by Gary King, et al., it works with [Zelig](https://zeligproject.org/), a separate framework that's designed to simplify modeling in R. With Zelig + Amelia, you can combine all of the $m$ imputations automatically with whatever Zelig uses for printing model results. I'm not a huge fan of Zelig, though, and I prefer using `lm()`, `glm()`, `stan_glm()`, and gang on my own, thank you very much.\n\nHowever, doing it on my own means there's a little more work involved with combining coefficients and parameters across imputations. Fortunately, the [tidyverse](https://www.tidyverse.org/)—specifically its ability to store models within data frames—makes it really easy to deal with models based on imputed data. Here's how to do it using tidy functions. The code for this whole process can be greatly simplified in real life. You technically don't need all these intermediate steps, though they're helpful for seeing what's going on behind the scenes. \n\nWe'll start by working with some basic example imputed data frame from Amelia's built-in data. We create 5 imputed datasets defining countries and years as cross sections and time series, and we log GDP per capita in the predictive model:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(Amelia)\nlibrary(broom)\n\nset.seed(1234)\ndata(africa)\nimp_amelia <- amelia(x = africa, m = 5, cs = \"country\", ts = \"year\", \n                     logs = \"gdp_pc\", p2s = 0)\n```\n:::\n\n\nThe resulting object contains a list of data frames, and each imputed dataset is stored in a list slot named \"imputations\" or `imp_amelia$imputations`. We can combine these all into one big data frame with `bind_rows()`, group by the imputation number ($m$), and nest them into imputation-specific rows:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# unclass() is necessary because bind_rows() will complain when dealing with\n# lists with the \"amelia\" class, which is what amelia() returns\nall_imputations <- bind_rows(unclass(imp_amelia$imputations), .id = \"m\") %>%\n  group_by(m) %>%\n  nest()\n\nall_imputations\n## # A tibble: 5 × 2\n## # Groups:   m [5]\n##   m     data              \n##   <chr> <list>            \n## 1 imp1  <tibble [120 × 7]>\n## 2 imp2  <tibble [120 × 7]>\n## 3 imp3  <tibble [120 × 7]>\n## 4 imp4  <tibble [120 × 7]>\n## 5 imp5  <tibble [120 × 7]>\n```\n:::\n\n\nWith this nested data, we can use `purrr::map()` to run models and return tidy summaries of those models directly in the data frame:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodels_imputations <- all_imputations %>%\n  mutate(model = data %>% map(~ lm(gdp_pc ~ trade + civlib, data = .)),\n         tidied = model %>% map(~ tidy(., conf.int = TRUE)),\n         glance = model %>% map(~ glance(.)))\n\nmodels_imputations\n## # A tibble: 5 × 5\n## # Groups:   m [5]\n##   m     data               model  tidied           glance           \n##   <chr> <list>             <list> <list>           <list>           \n## 1 imp1  <tibble [120 × 7]> <lm>   <tibble [3 × 7]> <tibble [1 × 12]>\n## 2 imp2  <tibble [120 × 7]> <lm>   <tibble [3 × 7]> <tibble [1 × 12]>\n## 3 imp3  <tibble [120 × 7]> <lm>   <tibble [3 × 7]> <tibble [1 × 12]>\n## 4 imp4  <tibble [120 × 7]> <lm>   <tibble [3 × 7]> <tibble [1 × 12]>\n## 5 imp5  <tibble [120 × 7]> <lm>   <tibble [3 × 7]> <tibble [1 × 12]>\n```\n:::\n\n\nHaving the models structured like this makes it easy to access coefficients for models from individual imputations, like so:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodels_imputations %>%\n  filter(m == \"imp1\") %>%\n  unnest(tidied)\n## # A tibble: 3 × 11\n## # Groups:   m [1]\n##   m     data     model  term     estim…¹ std.e…² stati…³  p.value conf.…⁴ conf.…⁵ glance  \n##   <chr> <list>   <list> <chr>      <dbl>   <dbl>   <dbl>    <dbl>   <dbl>   <dbl> <list>  \n## 1 imp1  <tibble> <lm>   (Interc…   114.    97.7     1.17 2.44e- 1   -79.0   308.  <tibble>\n## 2 imp1  <tibble> <lm>   trade       18.1    1.25   14.4  9.65e-28    15.6    20.6 <tibble>\n## 3 imp1  <tibble> <lm>   civlib    -631.   182.     -3.46 7.47e- 4  -993.   -270.  <tibble>\n## # … with abbreviated variable names ¹​estimate, ²​std.error, ³​statistic, ⁴​conf.low,\n## #   ⁵​conf.high\n```\n:::\n\n\nMore importantly, we can access the coefficients for all the models, which is essential for combining and averaging the coefficients across all five imputations.\n\nPooling or melding coefficients from many models is a little trickier than just averaging them all together (as delightfully easy as that would be). [Donald Rubin (1987)](https://doi.org/10.1002/9780470316696) outlines an algorithm/set of rules for combining the results from multiply imputed datasets that reflects the averages and accounts for differences in standard errors. Rubin's rules are essentially a fancier, more robust way of averaging coefficients and other quantities of interest across imputations.\n\nAmelia has a built-in function for using Rubin's rules named `mi.meld()` that accepts two m-by-k matrices (one for coefficients and one for standard errors) like so:\n\n```text\n      coef1  coef2  coefn\nimp1  x      x      x\nimp2  x      x      x\nimpn  x      x      x\n```\n\nWe can use some dplyr/tidyr magic to wrangle the regression results into this form:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Create a wide data frame of just the coefficients and standard errors\nparams <- models_imputations %>%\n  unnest(tidied) %>%\n  select(m, term, estimate, std.error) %>%\n  gather(key, value, estimate, std.error) %>%\n  spread(term, value) %>% \n  ungroup()\nparams\n## # A tibble: 10 × 5\n##    m     key       `(Intercept)` civlib trade\n##    <chr> <chr>             <dbl>  <dbl> <dbl>\n##  1 imp1  estimate          114.   -631. 18.1 \n##  2 imp1  std.error          97.7   182.  1.25\n##  3 imp2  estimate          123.   -626. 18.0 \n##  4 imp2  std.error          96.8   181.  1.24\n##  5 imp3  estimate          114.   -633. 18.2 \n##  6 imp3  std.error          96.5   181.  1.24\n##  7 imp4  estimate          119.   -651. 18.2 \n##  8 imp4  std.error          95.4   180.  1.22\n##  9 imp5  estimate          132.   -648. 18.0 \n## 10 imp5  std.error          95.2   180.  1.22\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Extract just the coefficients\njust_coefs <- params %>%\n  filter(key == \"estimate\") %>%\n  select(-m, -key)\njust_coefs\n## # A tibble: 5 × 3\n##   `(Intercept)` civlib trade\n##           <dbl>  <dbl> <dbl>\n## 1          114.  -631.  18.1\n## 2          123.  -626.  18.0\n## 3          114.  -633.  18.2\n## 4          119.  -651.  18.2\n## 5          132.  -648.  18.0\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Extract just the standard errors\njust_ses <- params %>%\n  filter(key == \"std.error\") %>%\n  select(-m, -key)\njust_ses\n## # A tibble: 5 × 3\n##   `(Intercept)` civlib trade\n##           <dbl>  <dbl> <dbl>\n## 1          97.7   182.  1.25\n## 2          96.8   181.  1.24\n## 3          96.5   181.  1.24\n## 4          95.4   180.  1.22\n## 5          95.2   180.  1.22\n```\n:::\n\n\nWe can then use these matrices in `mi.meld()`, which returns a list with two slots—`q.mi` and `se.mi`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncoefs_melded <- mi.meld(just_coefs, just_ses)\ncoefs_melded\n## $q.mi\n##      (Intercept) civlib trade\n## [1,]         121   -638  18.1\n## \n## $se.mi\n##      (Intercept) civlib trade\n## [1,]        96.7    181  1.24\n```\n:::\n\n\nArmed with these, we can create our regression summary table with some more dplyr wizardry. To calculate the p-value and confidence intervals, we need to extract the degrees of freedom from one of the imputed models\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_degree_freedom <- models_imputations %>%\n  unnest(glance) %>%\n  filter(m == \"imp1\") %>%\n  pull(df.residual)\n\nmelded_summary <- as.data.frame(cbind(t(coefs_melded$q.mi),\n                                      t(coefs_melded$se.mi))) %>%\n  magrittr::set_colnames(c(\"estimate\", \"std.error\")) %>%\n  mutate(term = rownames(.)) %>%\n  select(term, everything()) %>%\n  mutate(statistic = estimate / std.error,\n         conf.low = estimate + std.error * qt(0.025, model_degree_freedom),\n         conf.high = estimate + std.error * qt(0.975, model_degree_freedom),\n         p.value = 2 * pt(abs(statistic), model_degree_freedom, lower.tail = FALSE))\n\nmelded_summary\n##                    term estimate std.error statistic conf.low conf.high  p.value\n## (Intercept) (Intercept)    120.6     96.67      1.25    -70.9     312.0 2.15e-01\n## civlib           civlib   -637.8    181.13     -3.52   -996.6    -279.1 6.13e-04\n## trade             trade     18.1      1.24     14.63     15.6      20.5 3.45e-28\n```\n:::\n\n\nHooray! Correctly melded coefficients and standard errors!\n\nBut what do we do about the other model details, like $R^2$ and the F-statistic? How do we report those?\n\nAccording to [a post on the Amelia mailing list](https://lists.gking.harvard.edu/pipermail/amelia/2016-July/001249.html), there are two ways. First, we can use a fancy method for combining $R^2$ and adjusted $R^2$ described by [Ofer Harel (2009)](https://doi.org/10.1080/02664760802553000). Second, we can just take the average of the $R^2$s from all the imputed models. The results should be roughly the same.\n\nHarel's method involves two steps:\n\n1. In each complete data set, calculate the $R^2$, take its square root ($R$), transform $R$ with a Fisher z-transformation ($Q = \\frac{1}{2} \\log_{e}(\\frac{1 + R}{1 - R})$), and calculate the variance of $R^2$ (which is $\\frac{1}{\\text{degrees of freedom}}$)\n2. Meld the resulting $Q$ and variance using Rubin's rules (`mi.meld()`; this creates $Q_a$), undo the z-transformation ($R_a = (\\frac{-1 + \\exp(2Q_a)}{1 + \\exp(2Q_a)})^2$), and square it ($R_a^2$)\n\nThat looks complicated, but it's fairly easy with some dplyr magic. Here's how to do it for adjusted $R^2$ (the same process works for regular $R^2$ too):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Step 1: in each complete data set, calculate R2, take its square root,\n# transform it with Fisher z-transformation, and calculate the variance of R2\\\nr2s <- models_imputations %>%\n  unnest(glance) %>%\n  select(m, adj.r.squared, df.residual) %>%\n  mutate(R = sqrt(adj.r.squared),  # Regular R\n         Q = 0.5 * log((R + 1) / (1 - R)),  # Fisher z-transformation\n         se = 1 / df.residual)  # R2 variance\nr2s\n## # A tibble: 5 × 6\n## # Groups:   m [5]\n##   m     adj.r.squared df.residual     R     Q      se\n##   <chr>         <dbl>       <int> <dbl> <dbl>   <dbl>\n## 1 imp1          0.643         117 0.802  1.10 0.00855\n## 2 imp2          0.648         117 0.805  1.11 0.00855\n## 3 imp3          0.652         117 0.807  1.12 0.00855\n## 4 imp4          0.660         117 0.812  1.13 0.00855\n## 5 imp5          0.654         117 0.808  1.12 0.00855\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Step 2: combine the results using Rubin's rules (mi.meld()), inverse transform\n# the value, and square it\n\n# Meld the R2 values with mi.meld()\nQ_melded <- mi.meld(as.matrix(r2s$Q), as.matrix(r2s$se))\n\n# Inverse transform Q to R and square it\nr2_melded <- ((exp(2 * Q_melded$q.mi) - 1) / (1 + exp(2 * Q_melded$q.mi)))^2\nr2_melded\n##       [,1]\n## [1,] 0.651\n```\n:::\n\n\nThe correctly pooled/melded $R^2$ is thus 0.651. Neat.\n\nHow does this compare to just the average of all the $R^2$s from all the imputations?\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nr2s_avg <- models_imputations %>%\n  ungroup() %>% \n  unnest(glance) %>%\n  summarize(adj.r.squared_avg = mean(adj.r.squared)) %>%\n  pull(adj.r.squared_avg)\nr2s_avg\n## [1] 0.651\n```\n:::\n\n\nThe incorrectly averaged $R^2$ is 0.651, which is basically identical to the correctly melded 0.651. This is probably because the models from the five imputed models are already fairly similar—there might be more variance in $R^2$ in data that's less neat. But for this situation, the two approaches are essentially the same. Other model diagnostics like the F-statistic can probably be pooled just with averages as well. I haven't found any specific algorithms for melding them with fancy math. \n\nSo, in summary, combine the coefficients and standard errors from multiply imputed models with `mi.meld()` and combine other model parameters like $R^2$ either with Harel's fancy method or by simply averaging them.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
{
  "hash": "206c56eeca0092a086ed19a4a6bb4903",
  "result": {
    "markdown": "---\ntitle: \"How to create a(n almost) fully Bayesian outcome model with inverse probability weights\"\ndate: 2021-12-20\ndescription: \"Use a posterior distribution of inverse probability weights in a Bayesian outcome model to conduct (nearly) fully Bayesian causal inference with R, brms, and Stan\"\nimage: index_files/figure-html/plot-ate-posterior-1.png\nengine: knitr\ncategories:\n  - r\n  - tidyverse\n  - regression\n  - statistics\n  - data visualization\n  - causal inference\n  - do calculus\n  - DAGs\n  - bayes\n  - brms\n  - stan\ndoi: 10.59350/gyvjk-hrx68\ncitation: true\n---\n\n::: {.cell}\n<style type=\"text/css\">\n.first-block {\n    padding-right: 10px;\n}\n</style>\n:::\n\n\n\n\n::: {.callout-important}\n### Read the previous post first!\n\nThis post is a sequel to <a href=\"https://www.andrewheiss.com/blog/2021/12/18/bayesian-propensity-scores-weights/\">the previous one on Bayesian propensity scores</a> and won't make a lot of sense without reading that one first. Read that one first!\n:::\n\nIn [my previous post about how to create Bayesian propensity scores](https://www.andrewheiss.com/blog/2021/12/18/bayesian-propensity-scores-weights/) and how to legally use them in a second stage outcome model, I ended up using [frequentist models for the outcome stage](https://www.andrewheiss.com/blog/2021/12/18/bayesian-propensity-scores-weights/#a-legal-way-to-use-weights-bayesianly). I did this for the sake of computational efficiency—running 2,000 models with `lm()` is way faster than running 2,000 individual Bayesian models with `brm()` from **brms** (like, creating 2,000 Bayesian models could take hours or days or weeks!)\n\nI concluded with [this paragraph](https://www.andrewheiss.com/blog/2021/12/18/bayesian-propensity-scores-weights/#things-i-dont-know-yet-but-want-to-know):\n\n> We could technically run a Bayesian outcome model with `brm()`, but we'd have to run it 2,000 times—one model per set of weights—and that would take literally forever and might melt my computer. There could be a way to only run a single outcome model once and use one set of weights for each of the iterations (i.e. use the first column of propensity scores for the first iteration of the outcome model, the second for the second, and so on), but that goes beyond my skills with **brms**.\n\n[Jordan Nafa](https://twitter.com/adamjnafa) took on this challenge, though, and figured out a way to do exactly this with Stan!! He has a [super well-documented example at GitHub](https://github.com/ajnafa/Bayesian-IPW), and I've adapted and borrowed liberally from his code for this post. **Check out his example for additional updates and even more fantastic explanation!**\n\n\n## The intuition\n\nAfter creating a treatment model in the design stage (i.e. predicting net usage based on the confounders), we generate a matrix of propensity scores or inverse probability weights ($\\nu$ in @LiaoZigler:2020's approach). Each column of this matrix contains one posterior draw of propensity scores or weights. \n\n[In my original post](https://www.andrewheiss.com/blog/2021/12/18/bayesian-propensity-scores-weights/#a-legal-way-to-use-weights-bayesianly), we generated 2,000 sets of propensity scores and then ran 2,000 outcome models with `lm()` to calculate the average treatment effect (ATE).\n\n**brms** allows you to use weights with a slightly different syntax from `lm()`: \n\n```r\nbrm(bf(outcome | weights(iptw) ~ treatment), data = whatever)\n```\n\nThe argument we feed to `weights()` needs to be a column in the data that we're working with (like `iptw` here for the inverse probability weights).\n\nSince it's not really feasible to run thousands of separate Bayesian models, we can instead run the outcome model once and feed one set of weights for each MCMC iteration. Conceptually, we want to do something like this:\n\n- Outcome model posterior draw 1: `bf(outcome | weights(iptw[,1]) ~ treatment)`\n- Outcome model posterior draw 2: `bf(outcome | weights(iptw[,2]) ~ treatment)`\n- Outcome model posterior draw 3: `bf(outcome | weights(iptw[,3]) ~ treatment)`\n- (and so on)\n\nEach posterior draw thus gets its own set of weights, and all the draws are combined in the end as a single posterior distribution of the average treatment effect.\n\n\n## Using a matrix of weights with raw Stan code\n\nUnfortunately **brms** [can't handle a matrix of weights](https://discourse.mc-stan.org/t/how-to-update-a-brmsfit-object-with-a-modified-brms-generated-stan-model-marginalizing-over-a-distribution-of-weights/7567). The `weights()` term in the model formula can only take a single column—it can't iterate through a bunch of columns in a matrix. This is sad because it means we have to leave the comfort and convenience of **brms** and work with Stan code directly.\n\nFortunately, **brms** makes it easy to work with raw Stan and does most of the hard work for us. The `make_stancode()` function will convert a `brms`-based model into Stan code that we can then edit and tinker with.\n\nHere's how we can do it (again with heavily annotated code from [Jordan Nafa's phenomenal example of how this all works](https://github.com/ajnafa/Bayesian-IPW)):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)    # ggplot2, dplyr, %>%, and friends\nlibrary(brms)         # Nice interface for Bayesian models through Stan\nlibrary(rstan)        # For running Stan through R\nlibrary(tidybayes)    # For dealing with MCMC draws in a tidy way\nlibrary(ggdist)       # For distribution-related geoms\nlibrary(broom.mixed)  # For converting Stan-based model results to data frames\nlibrary(here)         # Convenient way for locating files\n\nset.seed(6348)  # From random.org\n\n# Use the Egypt palette from the MetBrewer package\negypt <- MetBrewer::met.brewer(\"Egypt\")\n\n# Custom ggplot theme to make pretty plots\n# Get Archivo Narrow at https://fonts.google.com/specimen/Archivo+Narrow\ntheme_nice <- function() {\n  theme_minimal(base_family = \"Archivo Narrow\") +\n    theme(panel.grid.minor = element_blank(),\n          plot.background = element_rect(fill = \"white\", color = NA),\n          plot.title = element_text(face = \"bold\"),\n          axis.title = element_text(face = \"bold\"),\n          strip.text = element_text(face = \"bold\", size = rel(0.8), hjust = 0),\n          strip.background = element_rect(fill = \"grey80\", color = NA),\n          legend.title = element_text(face = \"bold\"))\n}\n\n# Use this theme on all plots\ntheme_set(\n  theme_nice()\n)\n\n# Make all labels use Archivo by default\nupdate_geom_defaults(\"label\",\n                     list(family = \"Archivo Narrow\",\n                          fontface = \"bold\"))\n```\n:::\n\n\nFirst we'll load the data (see the [previous post](https://www.andrewheiss.com/blog/2021/12/18/bayesian-propensity-scores-weights/#basic-frequentist-example) or [my class example on inverse probability weighting](https://evalf21.classes.andrewheiss.com/example/matching-ipw/) for more details about what these columns are). **Our main question here is the effect of mosquito net usage on malaria risk.**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnets <- read_csv(\"https://evalf21.classes.andrewheiss.com/data/mosquito_nets.csv\")\n```\n:::\n\n\n\n\nWe'll then create a model that predicts whether people self-select into using a mosquito net, based on our confounders of income, health status, and nighttime temperatures.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/model-treatment_6b4f35cb5714b96e044335d90b61511d'}\n\n```{.r .cell-code}\n# First stage model predicting net usage with the confounders\nmodel_treatment <- brm(\n  bf(net ~ income + temperature + health,\n     decomp = \"QR\"),  # QR decomposition handles scaling and unscaling for us\n  family = bernoulli(),  # Logistic regression\n  data = nets,\n  chains = 8, cores = 8, iter = 2000,\n  seed = 1234, backend = \"cmdstanr\"\n)\n## Start sampling\n```\n:::\n\n\nWe'll the extract the predicted probabilities / propensity scores from the model and calculate inverse probability weights in each posterior draw for each person in the dataset:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/create-iptws_8bc6bffa54d58b4cfe137ecb9c1171b9'}\n\n```{.r .cell-code}\n# Extract posterior predicted propensity scores\npred_probs_chains <- posterior_epred(model_treatment)\n\n# Rows are posterior draws, columns are original rows in dataset\ndim(pred_probs_chains)\n## [1] 8000 1752\n\n# Create a matrix of weights where each column is a posterior draw\n# Transpose the matrix so that columns are posterior draws\nipw_matrix <- t(pred_probs_chains) %>%\n  as_tibble(.name_repair = \"universal\") %>% \n  # Add treatment column so we can calculate weights\n  mutate(net_num = nets$net_num) %>% \n  # Calculate weights\n  mutate(across(starts_with(\"...\"),\n                ~ (net_num / .x) + ((1 - net_num) / (1 - .x))\n  )) %>% \n  # Get rid of treatment column\n  select(-net_num)\n\n# Rows are original rows in data and columns are posterior draws\ndim(ipw_matrix)\n## [1] 1752 8000\n\nhead(ipw_matrix, c(5, 10))\n## # A tibble: 5 × 10\n##    ...1  ...2  ...3  ...4  ...5  ...6  ...7  ...8  ...9 ...10\n##   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1  2.49  3.01  2.50  2.62  2.66  2.78  2.73  2.74  2.60  2.70\n## 2  1.70  1.57  1.70  1.61  1.62  1.65  1.64  1.63  1.69  1.63\n## 3  1.22  1.16  1.22  1.17  1.21  1.17  1.14  1.14  1.22  1.20\n## 4  3.64  4.02  3.63  4.03  3.68  3.92  4.16  4.17  3.26  3.64\n## 5  1.44  1.44  1.45  1.41  1.45  1.44  1.43  1.43  1.58  1.47\n```\n:::\n\n\n\n### Export starter Stan code\n\nNext we need to use these weights in an outcome model using the neat trick of using one column of weights for each iteration of the outcome model, which requires actual Stan code (which I haven't really written since [this blog post](https://www.andrewheiss.com/blog/2019/01/29/diff-means-half-dozen-ways/#bayesian-analysis-directly-with-stan) or [this research project](https://www.andrewheiss.com/research/articles/chaudhry-heiss-ngos-philanthropy/) ([see here](https://github.com/andrewheiss/ngo-crackdowns-philanthropy/tree/master/inst/stan))).\n\nTo simplify life and make it so we don't have to really write any raw Stan code, we'll create the skeleton of our Bayesian outcome model with **brms**, but we don't actually run it—instead we'll pass the formula and priors to `make_stancode()` and save the underlying Stan code as a new file.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Outcome model\n# We can just use a placeholder weights(1) since the nets data doesn't have any\n# actual weights in it. It's okay bc we'll never actually run this model.\noutcome_formula <- bf(malaria_risk | weights(1) ~ net,\n                      family = gaussian)\n\noutcome_priors <- c(prior(\"student_t(3, 0, 2.5)\", class = \"Intercept\"),\n                    prior(\"normal(0, 2.5)\", class = \"b\"))\n\n# Generate Stan code for the outcome model\nmake_stancode(\n  formula = outcome_formula,\n  data = nets,\n  prior = outcome_priors,\n  save_model = \"original_stan_code.stan\"\n)\n```\n:::\n\n\n\n### Adjust the Stan code slightly\n\nNext we need to make some minor adjustments to the Stan code. You can also download a complete version of all these changes in this .zip file:\n\n- [<i class=\"fas fa-file-archive\"></i>&ensp;`modified-stan-stuff.zip`](modified-stan-stuff.zip)\n\nHere's what needs to change:\n\n**Change 1**: Declare two functions in the `functions` block. **The actual code for these two functions will live in a separate C++ file that we'll look at and make in just a minute.** These will let us keep track of the current iteration number of the MCMC chains. [By design, Stan doesn't let you see the current iteration number](https://discourse.mc-stan.org/t/is-it-possible-to-access-the-iteration-step-number-inside-a-stan-program/1871/8) (Stan code is designed to be stateless and not dependent on specific iteration numbers) but these two functions allow us to keep track of this ourselves. (These functions were originally [written by Louis at the Stan forum](https://discourse.mc-stan.org/t/is-it-possible-to-access-the-iteration-step-number-inside-a-stan-program/1871/23); [this StackOverflow answer](https://stackoverflow.com/a/68901076/120898) shows another example of them working in the wild)\n\n::: {.callout-tip}\nIf you're editing this file in RStudio, you'll likely see a syntax error after this block, with the complaint \"Function declared, but not defined.\" That's okay. We'll officially define these functions in an external file and inject it into this Stan code when compiling. You can ignore the error.\n::: \n\n:::: {.columns .column-page-inset-right}\n::: {.column .first-block width=\"45%\"}\n\n```{.stan}\n// Original block\n\nfunctions {\n}\n```\n\n:::\n::: {.column width=\"45%\"}\n\n```{.stan}\n// Modified block\n// ADD 2 NEW LINES\nfunctions {\n  void add_iter();  // ~*~THIS IS NEW~*~\n  int get_iter();  // ~*~THIS IS NEW~*~\n}\n```\n\n:::\n::::\n\n**Change 2**: Remove the declaration for the `weights` variable from the `data` block and add new declarations for two new variables: `L` for keeping track of the number of columns in the weights matrix, and `IPW` for the weights matrix:\n\n:::: {.columns .column-screen-inset-right}\n::: {.column .first-block width=\"45%\"}\n\n```{.stan}\n// Original block\n\ndata {\n  int<lower=1> N;  // total number of observations\n  vector[N] Y;  // response variable\n  vector<lower=0>[N] weights;  // model weights\n  int<lower=1> K;  // number of population-level effects\n  matrix[N, K] X;  // population-level design matrix\n  int prior_only;  // should the likelihood be ignored?\n}\n```\n\n:::\n::: {.column width=\"45%\"}\n\n```{.stan}\n// Modified block\n// REMOVE 1 LINE; ADD 2 NEW LINES\ndata {\n  int<lower=1> N;  // total number of observations\n  vector[N] Y;  // response variable\n  //vector<lower=0>[N] weights;  // model weights -- ~*~REMOVE THIS~*~\n  int<lower=1> K;  // number of population-level effects\n  matrix[N, K] X;  // population-level design matrix\n  int prior_only;  // should the likelihood be ignored?\n  int L;  // number of columns in the weights matrix -- ~*~THIS IS NEW~*~\n  matrix[N, L] IPW;  // weights matrix -- ~*~THIS IS NEW~*~\n}\n```\n\n:::\n::::\n\n**Change 3**: In the `model` block, add the ability to keep track of the current iteration with `get_iter()`. And—most importantly—modify the actual model so that it uses a column from the weights matrix `IPW[n, M]` rather than the single `weights[n]` vector. \n\n:::: {.columns .column-screen-inset-right}\n::: {.column .first-block width=\"45%\"}\n\n```{.stan}\n// Original block\n\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] mu = Intercept + Xc * b;\n    for (n in 1:N) {\n      target += weights[n] * (normal_lpdf(Y[n] | mu[n], sigma));\n    }\n  }\n  // priors including constants\n  target += normal_lpdf(b | 0, 2.5);\n  target += student_t_lpdf(Intercept | 3, 0, 2.5);\n  target += student_t_lpdf(sigma | 3, 0, 14.8)\n    - 1 * student_t_lccdf(0 | 3, 0, 14.8);\n}\n```\n\n:::\n::: {.column width=\"45%\"}\n\n```{.stan}\n// Modified block\n// ADD 2 LINES\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] mu = Intercept + Xc * b;\n    \n    int M = get_iter();  // get the current iteration -- ~*~THIS IS NEW~*~\n    vector[N] weights = IPW[, M];  // get the weights for this iteration -- ~*~THIS IS NEW~*~\n    \n    for (n in 1:N) {\n      target += weights[n] * (normal_lpdf(Y[n] | mu[n], sigma));\n    }\n  }\n  // priors including constants\n  target += normal_lpdf(b | 0, 2.5);\n  target += student_t_lpdf(Intercept | 3, 0, 2.5);\n  target += student_t_lpdf(sigma | 3, 0, 14.8)\n    - 1 * student_t_lccdf(0 | 3, 0, 14.8);\n}\n```\n\n:::\n::::\n\n**Change 4**: And finally in the `generated quantities` block, add the `add_iter()` function so that the iteration counter increases by one at the end of the draw.\n\n:::: {.columns .column-screen-inset-right}\n::: {.column .first-block width=\"45%\"}\n\n```{.stan}\n// Original block\n\ngenerated quantities {\n  // actual population-level intercept\n  real b_Intercept = Intercept - dot_product(means_X, b);\n}\n```\n\n:::\n::: {.column width=\"45%\"}\n\n```{.stan}\n// Modified block\n// ADD 1 LINE\ngenerated quantities {\n  // actual population-level intercept\n  real b_Intercept = Intercept - dot_product(means_X, b);\n  \n  add_iter();  // update the counter each iteration --  ~*~THIS IS NEW~*~\n}\n```\n\n:::\n::::\n\n### Define C++ functions for the counter\n\nBefore compiling this and running our shiny new model, we need to officially define our two new functions for keeping track of the MCMC iteration: `add_iter()` and `get_iter()`. We need to do this with C++ (But don't worry if you've never touched C++ before! This is my first time using it too!). Create a file named `iterfuns.hpp` (or whatever you want to call it) with this in it:\n\n```{.cpp filename=\"iterfuns.hpp\"}\n// Declare an integer to keep track of the iteration count\nstatic int itct = 1;\n\n// Increment the counter\ninline void add_iter(std::ostream* pstream__) {\n  itct += 1;\n}\n\n// Retrieve the current count\ninline int get_iter(std::ostream* pstream__) {\n  return itct;\n}\n```\n\nWhen we compile the Stan code, we'll inject this C++ code into it and our `add_iter()` and `get_iter()` functions will work.\n\n\n### Compile the Stan code\n\nBefore running the model and creating a bunch of MCMC chains, we need to compile this Stan code into a binary program that we can then use for MCMC. This is the same thing that happens when you use `brms()` and wait for a few seconds while it says `Compiling Stan program...`.\n\nTechnically we can do this by passing the file name of our modified Stan code to `stan_model()`, but **rstan** and/or Stan does stuff behind the scenes (storing compiled programs in `/tmp` or as hidden files somewhere on the computer) for caching purposes, and it can store older (and incorrect) versions of compiled models that can be hard to track down and get rid of. According to the documentation of `stan_model()`, we can force Stan to recompile the model every time and avoid this caching headache (it's seriously a headache!) by first explicitly converting the Stan code to C++ with `stanc()`, and then passing *that* code to `stan_model()`.\n\nSo to avoid caching issues, we'll do this in two steps. First we'll convert our modified Stan code to C++. The `allow_undefined = TRUE` option here is necessary because `add_iter()` and `get_iter()` are defined in an external file and Stan will complain because they're not formally defined here (similar to why RStudio complains). \n\nThen we'll feed this converted code to `stan_model()` and inject the C++ file with the counter functions in it. To reference an external file in C++, we have to include a line that looks like this:\n\n```cpp\n#include \"/full/path/to/iterfuns.hpp\"\n```\n\nInstead of hand-typing that, we'll use `here()` from the **here** package to automatically generate the absolute path, along with some extra newlines (`\\n`).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Convert the modified Stan code to C++\noutcome_c <- stanc(\"modified_stan_code.stan\",\n                   allow_undefined = TRUE)\n\n# Compile C++ified Stan to a binary model object\noutcome_model <- stan_model(\n  stanc_ret = outcome_c, \n  includes = paste0('\\n#include \"', here('iterfuns.hpp'), '\"\\n')\n)\n```\n:::\n\n\n\n\nAfter a few seconds, we should have a new `outcome_model` object. This doesn't contain any results or chains or anything. This is essentially a mini standalone program that's designed to take our nets data, our weights, and the priors that we specified.\n\n::: {.callout-tip}\n### Fun fact\n\nThis is essentially the difference between **brms** and **rstanarm**. **rstanarm** comes with a bunch of pre-compiled model programs like `stan_glm()`, so you don't have to compile anything yourself ever—you don't need to wait for the `Compiling Stan program...` message to finish like you do with **brms**. But that extra pre-compiled speed comes at the cost of flexibility—you can't run as many models or do as many neat extra things with **rstanarm** as you can with **brms**.\n:::\n\n\n### Run the model\n\nFinally, we can feed our data into the `outcome_model` object and run the MCMC chains. Unlike **brms**, we can't just feed it a formula and a data frame. Instead, we need to feed this model a list with each of the variables we declared in the data block of our code. The syntax is a little wonky, and the best way to get a feel for how this list needs to be structured is to use `make_standata()` and follow the same structure.\n\nImportantly, we need to use at least the same number of posterior chains as we have weights for (i.e. we can't have 2,000 sets of weights and use 8,000 chains in the outcome model since we'll run out of weights). Here we'll use the same amount (2,000 per chain, only 1,000 of which are kept due to the warmup phase).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Make a dataset of all the covariates and an intercept column\noutcome_covariates <- model.matrix(~ net_num, data = nets)\nhead(outcome_covariates)\n##   (Intercept) net_num\n## 1           1       1\n## 2           1       0\n## 3           1       0\n## 4           1       1\n## 5           1       0\n## 6           1       0\n\n# Make a list of all the required pieces for the data block in the Stan model\noutcome_data <- list(\n  N = nrow(nets),\n  Y = nets$malaria_risk,\n  K = ncol(outcome_covariates),\n  X = outcome_covariates,\n  L = ncol(ipw_matrix),\n  IPW = as.matrix(ipw_matrix),\n  prior_only = 0\n)\nstr(outcome_data)\n## List of 7\n##  $ N         : int 1752\n##  $ Y         : num [1:1752] 33 42 80 34 44 25 19 35 32 40 ...\n##  $ K         : int 2\n##  $ X         : num [1:1752, 1:2] 1 1 1 1 1 1 1 1 1 1 ...\n##   ..- attr(*, \"dimnames\")=List of 2\n##   .. ..$ : chr [1:1752] \"1\" \"2\" \"3\" \"4\" ...\n##   .. ..$ : chr [1:2] \"(Intercept)\" \"net_num\"\n##   ..- attr(*, \"assign\")= int [1:2] 0 1\n##  $ L         : int 8000\n##  $ IPW       : num [1:1752, 1:8000] 2.49 1.7 1.22 3.64 1.44 ...\n##   ..- attr(*, \"dimnames\")=List of 2\n##   .. ..$ : NULL\n##   .. ..$ : chr [1:8000] \"...1\" \"...2\" \"...3\" \"...4\" ...\n##  $ prior_only: num 0\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/run-final-model_af43c27a26530509f9dffb10c3f13c69'}\n\n```{.r .cell-code}\n# FINALLY run the model!\noutcome_samples <- sampling(\n  outcome_model, \n  data = outcome_data, \n  chains = 8, \n  iter = 2000,\n  cores = 8,\n  seed = 1234\n)\n```\n:::\n\n\n\n## Analyze the results\n\nThe results from `rstan::sampling()` behave pretty similarly to the output from **rstanarm** or **brms** (which isn't surprising, since both of those packages are just fancier frontends for Stan). We can print the results, for instance:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprint(outcome_samples)\n## Inference for Stan model: modified_stan_code.\n## 8 chains, each with iter=2000; warmup=1000; thin=1; \n## post-warmup draws per chain=1000, total post-warmup draws=8000.\n## \n##                 mean se_mean     sd     2.5%      25%      50%       75%     97.5% n_eff Rhat\n## b[1]            -9.8    0.01   1.06    -12.0    -10.4     -9.8     -9.16     -7.66  8120    1\n## Intercept       35.7    0.00   0.25     35.2     35.5     35.7     35.87     36.19  3020    1\n## sigma           13.8    0.00   0.22     13.4     13.7     13.8     13.92     14.22  3991    1\n## b_Intercept     39.5    0.01   0.47     38.6     39.2     39.5     39.81     40.46  5295    1\n## lp__        -14315.3    2.01 183.11 -14727.6 -14418.5 -14298.7 -14185.22 -14009.48  8325    1\n## \n## Samples were drawn using NUTS(diag_e) at Mon Nov 28 15:17:32 2022.\n## For each parameter, n_eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor on split chains (at \n## convergence, Rhat=1).\n```\n:::\n\n\nOr we can use `tidy()` from **broom.mixed**:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntidy(outcome_samples, conf.int = TRUE)\n## # A tibble: 4 × 5\n##   term        estimate std.error conf.low conf.high\n##   <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n## 1 b[1]           -9.80     1.06     -12.0     -7.66\n## 2 Intercept      35.7      0.253     35.2     36.2 \n## 3 sigma          13.8      0.216     13.4     14.2 \n## 4 b_Intercept    39.5      0.470     38.6     40.5\n```\n:::\n\n\n\n\nThe coefficients aren't nicely named and separated into fixed and random effects like they are in **brms** output, but we can still get the results out (and [we could rename them in the model if we wanted](https://discourse.mc-stan.org/t/recovering-coefficient-names/12598), or we can do that on our own as we work with the data). The main coefficient we care about here is the one for `net`, or `b[1]`. Based on this Bayesian outcome model, after incorporating the uncertainty from the posterior distribution of inverse probability weights, the ATE of using a net is -9.8, with an error of 1.06.\n\nThis is so cool! At the end of the previous post, after running 2,000 frequentist models and combining the ATEs and standard errors, we ended up with these results:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# ----------------------------------------------------------------------------------------------\n# Excerpt from https://www.andrewheiss.com/blog/2021/12/18/bayesian-propensity-scores-weights/\n# ----------------------------------------------------------------------------------------------\n# Combined average treatment effect\nmean(outcome_models$ate)\n## [1] -10.1\n\n# Combined standard errors with Rubin's rules (this is correct)\nrubin_se(outcome_models$ate, outcome_models$ate_se)\n## [1] 1.01\n```\n:::\n\n\nWe get comparable results with this Bayesian outcome model, only now we get to do fun Bayesian inference with the results!\n\nInstead of confidence intervals, we have credible intervals: given the data we have, there's a 95% chance that the ATE is between -12.01 and -7.66. We can see this in the posterior distribution of the ATE:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Make a long, tidy data frame of the posterior draws\noutcome_tidy <- outcome_samples %>% \n  tidy_draws() %>% \n  rename(ate = `b[1]`)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(outcome_tidy, aes(x = ate)) +\n  stat_slab(aes(fill_ramp = after_stat(cut_cdf_qi(cdf, .width = c(0.02, 0.8, 0.95, 1)))),\n            fill = egypt[2]) +\n  scale_fill_ramp_discrete(range = c(1, 0.2), guide = \"none\") +\n  labs(x = \"Average treatment effect of using a mosquito net\", y = NULL,\n       caption = \"Median shown with vertical line; 80% and 95% credible intervals shown with shading\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-ate-posterior-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nWe can calculate the proportion of this posterior distribution that is less than zero to find the [*probability of direction*](https://easystats.github.io/bayestestR/articles/probability_of_direction.html), or the probability that the ATE is negative:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Find the proportion of posterior draws that are less than 0\noutcome_tidy %>% \n  summarize(prop_lessthan_0 = sum(ate < 0) / n())\n## # A tibble: 1 × 1\n##   prop_lessthan_0\n##             <dbl>\n## 1               1\n```\n:::\n\n\nNot surprisingly, given the distribution we saw above, there's a 100% chance that the ATE is negative.\n\nWe can also calculate the proportion of the posterior distribution that falls within a [*region of practical equivalence* (or ROPE)](https://easystats.github.io/bayestestR/articles/region_of_practical_equivalence.html). We can think of this as a \"dead zone\" of sorts. If the ATE is 0, we know for sure that there's no effect. If the ATE is something small like -0.51 or 0.73, we probably don't actually care—that's not a huge effect and could just be because of measurement error. If the ATE is sizable and far away from this \"dead zone\" / ROPE, we can be pretty confident of the substantiality of the effect.\n\nThere are a lot of ways to determine the size of the ROPE. You can base it on experience with the phenomenon (e.g. you're an expert in public health and know that a 1-2 point change in malaria risk scores is small, while a 10+ change is big), or you can base it on the data you have (like `0.1 * sd(outcome)`, [which is a common approach](https://easystats.github.io/bayestestR/articles/region_of_practical_equivalence.html#how-to-define-the-rope-range-)). \n\nFor this example, we'll pretend that any effect between −7 and 7 doesn't matter—for an expert, values like 2.4 or −6 would be negligible. This is a ***huge*** ROPE, by the way! If we follow the 0.1 × the standard deviation of the outcome rule, the ROPE should only be ±`0.1 * sd(nets$malaria_risk)`, or ±1.546. I'm using ±7 here just for the sake of illustration—I want to see the ROPE on the plot :)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Find the proportion of posterior draws that are less than 0\nprop_outside <- outcome_tidy %>% \n  summarize(prop_outside_rope = 1 - sum(ate >= -7 & ate <= 7) / n())\nprop_outside\n## # A tibble: 1 × 1\n##   prop_outside_rope\n##               <dbl>\n## 1             0.992\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(outcome_tidy, aes(x = ate)) +\n  stat_halfeye(aes(fill_ramp = after_stat(x >= 7 | x <= -7)), \n               fill = egypt[2], .width = c(0.95, 0.8)) +\n  scale_fill_ramp_discrete(from = egypt[1], guide = \"none\") +\n  annotate(geom = \"rect\", xmin = -7, xmax = 0, ymin = -Inf, ymax = Inf, \n           fill = egypt[1], alpha = 0.3) +\n  annotate(geom = \"label\", x = -3.5, y = 0.75, label = \"ROPE\\n(dead zone)\\n0 ± 7\") +\n  labs(x = \"Average treatment effect of using a mosquito net\", y = NULL,\n       caption = \"Median shown with point; 80% and 95% credible intervals shown with black bars\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-rope-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nUsing a gigantic ROPE of 0±7 malaria risk points, we can see that 99.2% of the posterior distribution lies outside the region of practical equivalence, which provides pretty strong evidence of a nice big ATE. This program definitely has an effect! (It's fake data! I made sure it had an effect!)\n\n\n## Using the results with **brms**\n\nOne issue with using `rstan::sampling()` instead of **brms** is that we can't do nice things like automatic extraction of posterior expectations or predictions, since the MCMC samples we have are a `stanfit` object and not a `stanreg` object (which is what both **rstanarm** and **brms** produce):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nposterior_epred(outcome_samples)\n## Error in UseMethod(\"posterior_epred\"): no applicable method for 'posterior_epred' applied to an object of class \"stanfit\"\n```\n:::\n\n\nHowever, it is possible to [create an empty **brms** object and stick the MCMC samples we made](https://github.com/paul-buerkner/brms/issues/682#issuecomment-501304142) with `rstan::sampling()` into it, which then allows us to do anything a regular **brms** model can do! This [only works when making minimal changes to the Stan code](https://discourse.mc-stan.org/t/creating-a-brmsfit-object-with-a-modified-brms-generated-stan-model/6840/2)—we can't modify the likelihood (or the `target` part of the `model` block of the Stan code). We didn't touch this part, though, so we can safely stick these samples into a **brms** object. \n\nTo do this, we first have to create an empty model by using the `empty = TRUE` argument. We also have to create a placeholder weights column, even though we're not actually fitting the model—**brms** will complain otherwise.\n\nWe can then assign the `outcome_samples` object to the `$fit` slot of the empty model. Finally, we have to change the coefficient names to be nicer and compatible with **brms**. Note how earlier the coefficient for `net_num` was named `b[1]`, which was inconvenient. By using `brms::rename_pars()`, we can change those subscripted/indexed names into nice names again.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Placeholder outcome model\noutcome_brms <- brm(bf(malaria_risk | weights(iptw) ~ net_num),\n    # brms needs a column for the weights, even though we're not\n    # fitting the model, so create a placeholder column of 1s\n    data = nets %>% mutate(iptw = 1),\n    empty = TRUE\n)\n\n# Add the samples from rstan::sampling() into the empty brms object and fix the\n# coefficient names (i.e. b[1] → b_net_num)\noutcome_brms$fit <- outcome_samples\noutcome_brms <- rename_pars(outcome_brms)\n```\n:::\n\n\nThe `outcome_brms` object is now just like any regular **brms** model, and everything works with it:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# It works!\nsummary(outcome_brms)\n##  Family: gaussian \n##   Links: mu = identity; sigma = identity \n## Formula: malaria_risk | weights(iptw) ~ net_num \n##    Data: nets %>% mutate(iptw = 1) (Number of observations: 1752) \n##   Draws: 8 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 8000\n## \n## Population-Level Effects: \n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept    39.51      0.47    38.59    40.46 1.00     5341     6208\n## net_num      -9.80      1.06   -12.01    -7.66 1.00     8048     6964\n## \n## Family Specific Parameters: \n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma    13.79      0.22    13.37    14.22 1.00     3986     4864\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n# Predicted values work\nepreds <- posterior_epred(outcome_brms)\nhead(epreds, c(5, 10))\n##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n## [1,] 30.3 39.1 39.1 30.3 39.1 39.1 30.3 39.1 39.1  39.1\n## [2,] 30.1 39.1 39.1 30.1 39.1 39.1 30.1 39.1 39.1  39.1\n## [3,] 30.1 39.2 39.2 30.1 39.2 39.2 30.1 39.2 39.2  39.2\n## [4,] 29.5 39.8 39.8 29.5 39.8 39.8 29.5 39.8 39.8  39.8\n## [5,] 30.0 39.4 39.4 30.0 39.4 39.4 30.0 39.4 39.4  39.4\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# posterior predictive checks work!\npp_check(outcome_brms)\n## Using 10 posterior draws for ppc type 'dens_overlay' by default.\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/ppcheck-brms-1.png){fig-align='center' width=90%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# emmeans works too!\nlibrary(emmeans)\noutcome_brms %>%\n    emtrends(~net_num, var = \"net_num\")\n##  net_num net_num.trend lower.HPD upper.HPD\n##        0          -9.8       -12     -7.65\n##        1          -9.8       -12     -7.65\n## \n## Point estimate displayed: median \n## HPD interval probability: 0.95\n```\n:::\n\n\n\n## Only tiny downside\n\nThere's just one minor issue with this approach: all the sampling has to be done with **rstan**. For whatever reason, **cmdstanr** doesn't like the iteration tracking functions and it crashes. This might be because [**cmdstanr** is pickier about C++ namespaces](https://discourse.mc-stan.org/t/custom-c-using-cmdstanr/19528/7)? I'm not sure. If it did work, the code would look something like this:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(cmdstanr)\n\noutcome_model_cmdstanr <- cmdstan_model(\n  stan_file = \"modified_stan_code.stan\",\n  cpp_options = list(USER_HEADER = here('iterfuns.hpp')),\n  stanc_options = list(\"allow-undefined\")\n)\n\noutcome_samples_cmdstanr <- outcome_model_cmdstanr$sample(data = outcome_data)\n```\n:::\n\n\nIn spite of this, being able to use Bayesian models in both the treatment/design stage and the outcome/analysis stage is incredibly powerful!\n\n\n## References\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}